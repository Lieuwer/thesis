\documentclass{article}
% \documentstyle{article}
\usepackage{comment}
\begin{comment}
use info from ktpfa

Updata question, method and related research to new idea

Include PFA model with single skill?
\end{comment}

\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{#1}}

\begin{document}
\title{Master Thesis Research Proposal}
\author{Lieuwe Rekker}
\maketitle
\nocite{labelcombi}
\nocite{lftransfer}
\nocite{offtaskmodel}
\nocite{POKS1}
\nocite{matrixfact}
\nocite{importance}
\nocite{knowledgeproblem}
\nocite{modelreview}
\nocite{engagement}
\nocite{engageproficiency}
\nocite{eirt}
\nocite{pfa}
\nocite{ktpfa}

\section{Introduction}
Many current intelligent tutor systems (ITS) use learner models that can give indications of to what extent a student has mastered a particular skill and even how fast students learn. The quality of these models is generally measured by how well these models predict 'one question (item) into the future' performance, which is also what algorithms are trying to maximize. Parameter values from these models are also inspected and stated to represent students' knowledge levels, how fast students learn, how difficult questions are etc. It would thus be wise to inspect other factors beyond the accuracy of the model to gain some idea of whether they actually convey some stable real world factor or are rather part of a more black-box like model that performs well on its prediction task. \cite{knowledgeproblem} clearly brings up that this is indeed a problem: widely differing parameter settings can yield similar, (near) optimal performances. This proposal proposes research to explore if parameters of learner models hold meaning in real life.

The approach of this research is from the perspective that models resemble reality, but are necessarily not precise mirrors of it. Initially generated data will be used to see how well parameter values can be retrieved at all and to see what happens to parameters when there is a mismatch between model and 'reality' (i.e. training a model on data generated by a different model). In the case of real data, original parameters or models are unknown. Over multiple training runs on different data-sets, variation in parameter values can be inspected. High variation here indicates that (despite possible good prediction performance of the model as a whole) the parameter does not represent anything in reality.



\section{Related work}
In \cite{knowledgeproblem} Beck goes beyond investigating simply the accuracy of a model (knowledge tracing in this case) and also looks at the parameter values. In his research a big problem comes forward: multiple global maxima: there are multiple (very different) parameter settings that lead to similar, near-optimal accuracies. This shows that the trained model could have parameter values that although the performance of the model is good are otherwise meaningless.



\section{Research Question}
\label{sec:RQ}
The question I would like to answer in my masters thesis is: "Are parameter values of IRT based models meaningful?". There are many different factors that have an influence on the parameter values obtained. Therefor the question is further split to look at the influence of particular factors. Also two different perspectives are taken: looking at generated data and looking at data obtained from real students.

The first factor taken into account is the amount of data. Preferably data is increased up to the point where the parameter estimates (or their variance) stabilizes, so that lack of data won't be an issue in further experiments. The second factor that will be looked at is the usage of different models representing different assumptions on the domain. The models looked at are Learning Factor Analysis (LFA), Performance Factor Analysis (PFA) and extended Item Response Theory (eIRT). Additionally a complex model will be used that combines all the models. The third factor will concern the way in which knowledge components for multi-skill items are combined. The final factor will look at the influence of 'domain'. Learning may work differently for different subjects and also the way a particular ITS structures its items can have an influence. For this factor a few different data-sets will be used.

The first angle taken in this research is to train models based on generated data. This way the true parameter values are known and an upper bound can be established for how well this particular model can retrieve its parameters. Additionally comparing how models do on data generated by other models might provide some insights into what happens to parameter estimates when the 'true' model and the trained model differ on known dimensions. The second angle is training the models on real data. In this case there won't be a ground truth learning rate, but rather the variance of parameter estimates on different sub-sets of the data can be compared with each other and the results from the first angle's experiments.


\section{Research Priorities}
Below are my priorities for the research.

{\bf Must} Use the different IRT models on two domains. Study their performance both on artificial and real data. Examine amount of data and method in which skills are combined.

{\bf Should} Add another domain

{\bf Could} Compare to a different kind of model to see if differences are more outrageous. Knowledge tracing seems preferred right now.

{\bf Would have} ?

\section{Method}




\subsection{Model Performance}

\label{sec:perf}
The most common performance measure used is some measure of accuracy of model predictions on next-item student performance. Although this research looks into the values of model parameters and is less concerned with other measures of performance such a measure will still be used. Maximizing this accuracy is what models are build to do and thus it plays an important role in the fitting of parameters. It will be interesting to see how this accuracy, which is optimized in modelling, behaves compared to how well parameters are estimated.

The specific accuracy measure used for this research will be A'. For this measure two items are represented to the model, one which was answered correctly and one which was answered incorrectly. The model is used to determine which is which. The advantage of this method is that "values of A' are statistically comparable across models and data sets" \cite{modelreview}.

\begin{comment}
Since the learner models are also used as an approximation of how students learn and as such influence decisions made on what exercises are given to a student, what exercises are included in general and what topics require more attention, the validity of the specific parameters of the model need to taken into account. In this research external validity (e.g. see if the parameter settings seem realistic) is not the concern, but rather whether the found parameters are consistent over multiple trainings of the model and (in the case of generated data) whether the training algorithm recovers the used parameters correctly.
\end{comment}

In this research the performance of interest is how meaningful the parameter values of the used models are. This will not be done by looking at the external validity of the values found through experimentation with students, but rather by looking at the stability of the values found from real data and the deviance from the true values in the case of generated data. If the values deviate much from the values used to create the data, these parameter values can be deemed not to measure correctly what they say they do. Also if values obtained from training on real data vary a lot, the values obtained are quite meaningless. The models used have some differences in what parameters are used exactly. In each models' paragraph it will therefore be described how the values (and variance) of its parameters can be compared to the values (and variance) of parameters of the models discussed before it.
\todo{Move part of this paragraph to RQ?}


\subsection{Data Folding}
Doing multiple training runs for every model means that the real data will be split into parts or that the runs will be done on partially overlapping data. Some consideration needs to be spend on balancing the number of runs, the possible amount of data per run and overlap of data between runs and what the consequences will be (e.g. variance of parameter values trained on large data sets may be less accurate or lower due to using less runs or overlapping data respectively)

In increasing the data the number of students and the number of problems and knowledge components are held constant. This is to ensure that the number of parameters that are fitted will remain the same over the different test runs.

\subsection{Domains}
\label{sec:domain}
In generating the data there are many other factors that might play a role in the performance of the models. Among those factors are the values that the parameters will be given, the distribution of knowledge components over the items, the ratio of students to items etc. In order to make the generated data experiments resemble reality, the experiments (or at least some) on real data should be performed first to use values from those experiments for the generated data. In the same way structural elements (such as distribution of knowledge components over items) will be lifted from the data. This will be done for every data set used. These settings each represent a different domain.

\subsection{Experiments}
In synopsis the research will contain the following experiments/steps
\begin{enumerate}
\item Generated data
\begin{enumerate}
\item Do preliminary runs with every model on every domain to obtain 'domain parameter values' (see \ref{sec:domain}
\item Generate data using every model. Train each model on each dataset using increasing amounts of data. Compare results (see \ref{sec:perf}
\item Generate data using every model and an alternative way of combining knowledge componenets (see \ref{sec:LFA}
\end{enumerate}
\item Train each model on the real data sets, also using increasing amounts of data and compare variances of parameters (see \ref{sec:perf}
\end{enumerate}


\section{Models}
All models used in this research are based on item response theory and use a version the 2 parameter item response function (IRF). The basis of this function is the logistic ogive function \ref{eq:logistic}. The most basic 2 parameter function is shown in \ref{eq:irt}. Here i stands for item and s stands for student. $\alpha$ is the discriminatory power of the item i.e. how much knowledge matters in how well a student will do on that item. $\beta$ is the difficulty of the item i.e. how high a students skill should be to achieve a P of .5. $\theta$ is the skill of a student, indicating how well the student can perform in this skill. The value of the function is interpreted as the probability that the student will answer the question correctly. This is the same interpretation that is used throughout all models used in this research.

\begin{equation}
\label{eq:logistic}
\sigma(x) = \frac{1}{1+e^-x}
\end{equation}

\begin{equation}
\label{eq:irt}
P = \sigma(\alpha_{i} (\theta_{s} - \beta_{i}))
\end{equation}

All models used in this research employ an important extension: it incorporates learning by students. Skill, $\theta$ is split up into an initial part and a learning rate, so that each time a question is answered the skill of the student increases. In the ITS the items belong to many different knowledge components and thus the parameters are fit per knowledge component. Depending on the data-set used an item can have multiple components associated with them, creating the necessity to combine multiple item response functions.

\todo{Say something about the fitting procedure.
Add paragraph on how to do comparisons with other models.}

\subsection{Learning Factor Analysis (LFA)}
\label{sec:LFA}
\todo{find and cite first paper where LFA was introduced and cite Learning Factor Analysis was first introduced by}


LFA uses a simplified version of the IRF, but extents it by introducing a learning rate as discussed in the introduction and by allowing multiple knowledge components to be associated with a single item.

\begin{equation}
P = \sigma(\sum_{c \in KC} \theta_{s,0} + \eta_{c} t_{s,c} - \beta_{c})
\end{equation}

The simplification of the model is done by dropping $\alpha$ (this model is called the Rasch model). The splitting of $\theta$ leads to the introduction of an initial skill $\theta_{0}$ defined per student, a learning rate $\eta$ defined per KC (i.e. the KC determines how fast or slow learning occurs) and a number of times that a student has seen items belonging to this particular knowledge component $t_{s,c}$. Please note that in the the original LFA $\beta$ is added. It is subtracted here to maintain similarity to the original IRF. This will have no other effect than that the signs for $\beta$ will be reversed.

Something more can be said about the way KCs are combined. Using a sum over the different KCs has shown to be usable in practice (\todo{find stuff said about it in papers}), but seems a bit odd in theory. Why would it be that an item that has a single difficult KC associated with it ($\beta >> \theta$) be harder to solve than an item that has some additional easy KCs associated with it (where $\beta < \theta$). For practical reasons it is difficult to train a model that will take the KC for which the term in the sum is minimal (in this case the term is $\theta_{s,0} + \eta_{c} t_{s,c} - \beta_{c}$), but this model will be used in an experiment to see its effect on every model in what happens to the parameter estimates.


\subsection{Performance Factor Analysis (PFA)}
PFA is a direct extension of LFA. Separate learning rates are used for questions answered correctly and questions answered incorrectly. Additionally $\theta_{0}$ is dropped.

\begin{equation}
P = \sigma(\sum_{c \in KC}  \gamma_{c} g_{s,c} + \rho_{c} f_{s,c} - \beta_{c})
\end{equation}

Here $\gamma$ is the learning rate for correct answers and g is the number of questions answered correctly. Consequently $\rho$ is the learning rate for incorrect answers and f is the number of questions answered incorrectly. Here too the sign for $\beta$ was reversed compared to the original.

To compare the PFA parameter values to LFA values, the weighted average (according to the ratio g:f for each problem) of $\gamma$ and $\rho$ should be compared to $\eta$. $\theta$ cannot be compared when comparing PFA on LFA data, vice versa, $\theta$ should be compared to 0.

\todo{
\cite{lftransfer} -> difficulty per item, use as well? Find why theta was dropped.
Paper that did include $\theta$, noting interesting seemed to happen there, still mention? ->include after all, otherwise there is too much of a difference with other models. Total models goes up to 5 then}

\subsection{extended Item Response Theory (eIRT)}
\label{sec:eirt}
The extended Item Response Theory model by Roijers et al \cite{eirt} is the most straightforward extension to the standard IRT model.

\begin{equation}
\label{eq:eirt}
P = \sigma(\alpha_{c} (\theta_{s,0} + \eta_{s} t_{s,c} - \beta_{c}))
\end{equation}

Similar to LFA $\theta$ is replaced by initial skill and a learning rate. Here the learning rate is taken per student though rather than per knowledge component as is done in LFA and PFA. Since $\theta_{0}$ and $\eta$ are defined over students, one could argue that $\alpha$ now represents a difficulty to learn a knowledge component, rather than simply its discriminatory power.


eIRT as defined by Roijers et al does not incorporate multiple skill steps. In order to be trained on multi-skill data and to be similar to the other models used \ref{eq:sumeirt} will be used as a multi-skill extension of the eIRF. To distinguish this version from the original eIRT, this extended version will be denoted as seIRT.

\begin{equation}
\label{eq:sumeirt}
P = \sigma(\sum_{c \in KC} \alpha_{c}(\theta_{s,0} + \eta_{s} t_{s,c} - \beta_{c}))
\end{equation}

\todo{Say something about the linear dependencies stuff. Actually I have no clue on the correct mathematical vernacular, so please nudge me to the right terms to make this more understandable/consice}
It should be noted that different parameter settings can lead to exactly the same model. E.g. all $\theta$s and $\beta$s could be increased by the same amount and the model would still be the same. To still be able to compare parameter values and variances the parameters should be normalized as follows. Take the lowest $\theta$ value to be 0 as to resolve the dependency with $\beta$ values. Take the highest $\theta$ value to be 1 as to fix the dependency between $\alpha$ and the other variables.

In comparing seIRT parameter values to those of LFA $\beta$ should be multiplied by $\alpha$. $\theta$ is to be multiplied by a weighted average of $\alpha$ (according to the ratio of KCs of the questions that the corresponding student has answered). Finally a weighted average of $\eta$ per KC should be taken according to the ratio of questions each student has answered containing that KC. seIRT can be compared to PFA by combining the steps above with the steps needed to compare PFA to LFA.

\subsection{Combined Model}
The three models introduced above can all be encompassed by a more complex model.

\begin{equation}
P = \sigma(\sum_{c \in KC} \alpha_c \theta_{s,0}+\eta_{s} \gamma_{c} s_{s,c} + \eta_{s}\rho_{c} f_{s,c} - \beta_{c})
\end{equation}

LFA can be obtained from this model by taking $\alpha=1$, $\eta=1$ and $\gamma=\rho$. PFA can be obtained from this model by taking $\alpha=1$, $\eta=1$ and $\theta_{0}=0$. The adapted eIRF can be obtained by taking $\gamma=\rho=\alpha$ and realizing that $\beta$ already incorporates $\alpha$.


\todo{
This section also needs the comparison stuff, but that shouldn't be too difficult. Quite similar to the ones above, just a bit more complicated
\section{Outstanding issues}
\begin{itemize}
\item Describe terms and use those consistently
\item 1-skill per item setting or multiple skill per item setting? -> depends on the next point, but it seems many are multi-dimensional
\item find good data-set(s)...
\end{itemize}
}
\bibliographystyle{alpha}   % this means that the order of references
			    % is determined by the order in which the
			    % \cite and \nocite commands appear
\bibliography{litlist}
\newpage
\appendix
\section{Glossary}
\todo{
Making a start with using terms consistently, plus I generally feel that a glossary would have helped me greatly in understanding papers etc.}
\begin{description}
  \item[Item] A problem (step) in the ITS to which a single answer can be given
  \item[Knowledge Component] A skill, piece of knowledge etc. that is associated with one or more items and in which students can have a level of competence
  \item[Question] An instance of an item
  \item[Skill] Level of an instance of a knowledge component for a particular student
\end{description}

\end{document}

