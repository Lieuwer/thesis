\documentclass{article}
% \documentstyle{article}
\usepackage{comment}
\usepackage{amsmath}
\begin{comment}

Include PFA model with single skill?
Include PFA model with difficulty per item? -> prob not
\end{comment}

\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{#1}}

\begin{document}
\title{Master Thesis Research Proposal}
\author{Lieuwe Rekker}
\maketitle
\nocite{labelcombi}
\nocite{lftransfer}
\nocite{offtaskmodel}
\nocite{POKS1}
\nocite{matrixfact}
\nocite{importance}
\nocite{knowledgeproblem}
\nocite{modelreview}
\nocite{engagement}
\nocite{engageproficiency}
\nocite{eirt}
\nocite{pfa}
\nocite{ktpfa}
\nocite{skillcombi}
\nocite{lfa}

\section{Introduction}
Many current intelligent tutor systems (ITS) use learner models that can give indications of to what extent a student has mastered a particular skill and even how fast students learn. The quality of these models is generally measured by how well these models predict 'one question into the future' performance, which is what these algorithms are made to maximize. Parameter values from these models are also inspected and stated to represent students' knowledge levels, how fast students learn, how difficult questions are etc. It would thus be wise to inspect other factors beyond the accuracy of the model to gain some idea of under what conditions they actually convey some stable real world factor or are rather part of a more black-box like model that performs well on its prediction task, but whose parameters are otherwise meaningless. \cite{knowledgeproblem} shows that this is indeed a problem: widely differing parameter settings can yield similar, (near) optimal performances. This proposal proposes research to explore if parameters of learner models hold meaning in real life.

The approach of this research is from the perspective that models resemble reality, but are necessarily not precise mirrors of it. Initially generated data will be used to see how well parameter values can be retrieved at all and to see what happens to parameters when there is a mismatch between model and 'reality' (i.e. training a model on data generated by a different model). The amount of training data will be looked at especially, since even if stable parameter values can be found, this will probably break down at some point. In the case of real data, original parameters and models are unknown. Over multiple training runs on different data-sets, variation in parameter values can be inspected. High variation here indicates that (despite possible good prediction performance of the model as a whole) the parameter does not represent anything in reality.



\section{Related work}
In \cite{knowledgeproblem} Beck goes beyond investigating simply the accuracy of a model (knowledge tracing in this case) and also looks at the parameter values. In his research a big problem comes forward: multiple global maxima: there are multiple (very different) parameter settings that lead to similar, near-optimal accuracies. Beck concludes from this that parameter values should be checked for plausibility, but does not raise further questions on the truthfulness of the model.

Learning factor analysis (LFA) is a cognitive model of how students learn. It is a Rasch model extended with a learning rate to model learning over time. In \cite{lfa} LFA is used in the context of a greater cognitive model that includes what knowledge components are linked to what items. The quality of fit (measured as the log likelihood of the entire data set given the model plus some penalty for the number of parameters) is used as a measure of quality of a specific linkage of knowledge components to items.
Performance factor analysis (PFA) is a further extension to LFA. It is introduced in \cite{pfa} as an alternative to knowledge tracing and focuses more on correctly estimating whether a student has mastered a particular knowledge component.
In \cite{ktpfa} Gong et al. also made a comparison between various knowledge tracing approaches and PFA. Whether PFA or KT performs better remains inconclusive, but they indicate that bounding parameter values in PFA leads to better results. Moreover they used a pretest and correlated the performance on this test to the initial knowledge parameter from the model. In this set-up an adapted version of PFA (the original doesn't use initial student knowledge) showed the highest correlation. (mention plausibility somehow?)

In \cite{eirt} Roijers et al. extend the 2PL IRT function to include a learning rate. This new version is called extended IRT (eIRT). They perform experiments using generated data, that shows that parameters can often be retrieved from the data. Additionally they have a real life data-set based on 14 students from three different groups. With the small amount of data in their real life set, they conclude based on their previous experiment that only rule difficulty and student initial knowledge can be determined with some accuracy. After training the model on the data the fitted initial knowledge of the three different groups confirmed their hypothesis of the ordering of average initial knowledge per group. Also the ordering of difficulty of the rules involved is indistinguishable to rankings made by experts. The authors thus showed that the difficulty and initial knowledge parameters obtained reflect the ordering of these in real life.


\section{Research Question}
\label{sec:RQ}
The question at the basis of the proposed research is "When are parameter values found in fitting IRT based models to data meaningful?". There are many different factors that have an influence on the parameter values obtained. Therefor the question is further split to look at the influence of particular factors. Also two different perspectives are taken: looking at generated data and looking at data obtained from real students. Finally accuracy of next-question performance predictions of the models are inspected as well, since these are generally looked at since these are generally in ITSs taken as a measure of quality and might provide some answer to the research question.

The first factor under consideration will be the amount of data. Preferably data is increased up to the point where the parameter estimates and their variance stabilize, so that a theoretical best achievable result can be established. The second factor that will be looked at is the usage of different models representing different assumptions on the domain. The models looked at are Learning Factor Analysis (LFA), Performance Factor Analysis (PFA) and extended Item Response Theory (eIRT). Additionally a complex model will be used that combines all the models. The third factor examined will be the way in which knowledge components for multi-skill items are combined. The final factor researched will be 'domain': learning may work differently for different subjects, the way a particular ITS structures its items and the context in which an ITS is used. For this factor a few different real life data-sets will be used.

The first angle taken in this research is to train models based on generated data. This way the true parameter values are known and an upper bound can be established for how well this particular model can retrieve its parameters. Additionally comparing how models do on data generated by other models might provide some insights into what happens to parameter estimates when the 'true' model and the trained model differ on known dimensions. The second angle is training the models on real data. In this case there won't be a ground truth learning rate, but rather the variance of parameter estimates on different sub-sets of the data can be compared with each other and the results from the first angle's experiments.


\section{Research Priorities}
Below are my priorities for the research.

{\bf Must} Use the different IRT models on two domains. Study their performance both on artificial and real data. Examine amount of data and method in which skills are combined.

{\bf Should} Add another domain

{\bf Could} Compare to a different kind of model to see if differences are more outrageous. Knowledge tracing seems preferred right now.

{\bf Won't have} Examine in a structured way what influence the knowledge component to items linkage has on parameter values.

\section{Method}




\subsection{Model Performance}

\label{sec:perf}
The most common performance measure used in the context of ITS learning models is some measure of accuracy of model predictions on next-item student performance. Although this research looks into the values of model parameters and is less concerned with other measures of performance an accuracy measure will still be used. The accuracy of a model may hold some relationship to how well the parameters are matched. This would be especially useful as accuracy measures are relatively easy to obtain and are often already looked at in ITSs.

The specific accuracy measure used for this research will be A'. For this measure two items are represented to the model, one which was answered correctly and one which was answered incorrectly. The model is used to determine which is which. The advantage of this method is that "values of A' are statistically comparable across models and data sets" \cite{modelreview}.

Log likelihood of the data given a model is another measure that plays a large role in this specific context as this is what is maximized in determining the model data. Although the log likelihood value might not be very informative, they do give an indication of how well the model fits the data-set and in combination with the measured accuracy one can get an idea to what extent overfitting occurs.

\begin{comment}
Since the learner models are also used as an approximation of how students learn and as such influence decisions made on what exercises are given to a student, what exercises are included in general and what topics require more attention, the validity of the specific parameters of the model need to taken into account. In this research external validity (e.g. see if the parameter settings seem realistic) is not the concern, but rather whether the found parameters are consistent over multiple trainings of the model and (in the case of generated data) whether the training algorithm recovers the used parameters correctly.
\end{comment}

In this research the performance of interest is how meaningful the parameter values of the used models are. Foremost this will not be done by looking at the external validity of the values found through experimentation with students, but rather by looking at the stability of the values found from real data and the deviance from the true values in the case of generated data. If the values deviate much from the values used to create the data, these parameter values can be deemed not to measure correctly (or too inaccurately) what they say they do. Also if values obtained from training on real data vary a lot, the values obtained are quite meaningless. The models used have some differences in what parameters are used exactly. In each models' paragraph it will therefore be described how the values (and variance) of its parameters can be compared to the values (and variance) of parameters of the models discussed before it.

Although the primary way in which model parameters will be examined is described above, alternative methods will be used where applicable. Some data-sets contain additional information that can be used to more directly draw conclusions on whether found parameter values are meaningful. Two examples from the related research section are \cite{eirt} where expert opinions and an indication of what groups have higher initial knowledge were used and \cite{ktpfa} where a pre-test was used to indicate initial knowledge.

\subsection{Data Folding}
Doing multiple training runs for every model means that the real data will be split into parts that possibly partially overlap. Some consideration needs to be spend on balancing the number of runs, the possible amount of data per run and overlap of data between runs and what the consequences will be (e.g. variance of parameter values trained on large data sets may be less accurate or lower due to using less runs or overlapping data respectively)

In increasing the data the number of students and the number of problems and knowledge components are held constant. This is to ensure that the number of parameters that are fitted will remain the same over the different test runs.

\subsection{Domains}
\label{sec:domain}
In generating the data there are many other factors that might play a role in the performance of the models. Among those factors are the values that the parameters will be given, the distribution of knowledge components over the items, the ratio of students to items etc. In order to make the generated data experiments resemble reality, the experiments (or at least some) on real data should be performed first to use values from those experiments for the generated data. In the same way structural and statistical elements (such as linkage of knowledge components to items, distribution of question answered per student etc.) will be obtained from the data. This will be done for every data set used. These settings each represent a different domain.

\subsection{Experiments}
In synopsis the research will contain the following experiments/steps
\begin{enumerate}
\item Generated data
\begin{enumerate}
\item Do preliminary runs with every model on every domain to obtain 'domain parameter values' (see \ref{sec:domain})
\item Generate data using every combination of model and domain. Train each model on each dataset using increasing amounts of data. Compare results (see \ref{sec:perf})
\item Generate data using every combination of model and domain and the two alternative ways of combining knowledge components (see \ref{sec:comb})
\end{enumerate}
\item Train each model on the real data sets, also using increasing amounts of data and compare results (variances, log likelihoods and accuracies) to each other and the runs on generated data (see \ref{sec:perf})
\end{enumerate}


\section{Models}
All models used in this research are based on item response theory and use a version the 2 parameter item response function (IRF). The basis of this function is the logistic ogive function \ref{eq:logistic}. The most basic 2 parameter IRF is shown in \ref{eq:irt}. Here i stands for item and s stands for student. $\alpha$ is the discriminatory power of the item i.e. how much knowledge matters in how well a student will do on that item. $\beta$ is the difficulty of the item i.e. how high a students skill should be to achieve a P of .5. $\theta$ is the skill of a student, indicating how well the student has mastered a knowledge component. The value of the function is interpreted as the probability that the student will answer a question correctly. This is the same interpretation that is used throughout all models in this research.

\begin{equation}
\label{eq:logistic}
\sigma(x) = \frac{1}{1+e^-x}
\end{equation}

\begin{equation}
\label{eq:irt}
P = \sigma(\alpha_{i} (\theta_{s} - \beta_{i}))
\end{equation}

All models used in this research employ an important extension: it incorporates learning by students. Skill, $\theta$ is split up into an initial part and a learning rate, so that each time a question is answered the skill of the student increases. In the ITS there are many different knowledge components and thus the parameters are fit per knowledge component. Depending on the data-set used an item can have multiple components associated with them, creating the necessity to combine multiple item response functions.

\todo{Say something about the fitting procedure for each seperate model.}

\subsection{Learning Factor Analysis (LFA)}
\label{sec:LFA}


LFA uses a simplified version of the IRF, but extents it by introducing a learning rate as discussed in the introduction and by allowing multiple knowledge components to be associated with a single item. The combination of KCs is made by summing the learned part of knowledge and the difficulty of the KC for every KC that is linked to the item.

\begin{equation}
P = \sigma(\theta_{s,0} + \sum_{c \in KC}  \eta_{c} t_{s,c} - \beta_{c})
\end{equation}

The simplification of the model is done by dropping $\alpha$ (this model is called the Rasch model). The splitting of $\theta$ leads to the introduction of an initial skill $\theta_{0}$ defined per student, a learning rate $\eta$ defined per KC (i.e. the KC determines how fast or slow learning occurs) and a number of times that a student has seen items belonging to this particular knowledge component $t_{s,c}$. Please note that in the the original LFA $\beta$ is added. It is subtracted here to maintain similarity to the original IRF and ensure uniformity with the other models used. This will have no other effect than that the signs for $\beta$ will be reversed.

When looking at a data set where only a single knowledge component is linked to every item $\beta$ and $\theta_{0}$ are not independent: we can raise both by any amount. To solve this issue, the lowest value of $\theta_{0}$ will be set to 0.
\subsubsection{Combining knowledge components}
\label{sec:comb}
Something more can be said about the way KCs are combined. In \cite{skillcombi} Cen et al. show that in practice there is no difference in performance between an additive model (as used here) or a conjunctive model (where probabilities of individual KCs are multiplied). Nevertheless the authors already mention that this is probably the case because for most KCs $\beta < \eta_{c} t_{s,c}$, meaning that adding KCs does decrease the chance of answering the question correctly as would be expected. In their paper they already propose using a data set where ($\beta > \eta_{c} t_{s,c}$) to see if this is indeed why this way of combining KCs works well in practice.

The experiment proposed above will be put to the test here. Fitting a conjunctive model is hard in practice, but generating data using one is rather straightforward. Whether a real life data set contains many questions where skills are such that $\beta < \eta_{c} t_{s,c}$ cannot be said at this point. Even if this is not the case though, it can be argued that the parameter values can be skewed slightly to ensure that this occurs. The rationale behind is, is that the fitted values obtained from the real data may be skewed towards $\beta < \eta_{c} t_{s,c}$ due to a additive model being used in the fitting process. It would then be expected though that the retrieved parameters using these values would be skewed towards $\beta < \eta_{c} t_{s,c}$ again.

\subsection{Performance Factor Analysis (PFA)}
PFA is a direct extension of LFA. In PFA separate learning rates are used for questions answered correctly and questions answered incorrectly. Additionally $\theta_{0}$ is dropped. As put forward in \cite{pfa} $\theta_{0}$ is dropped because this extension is made mostly to make this model more useful in ITSs. Leaving out any student specific parameter makes the model more easily applicable to students not used in the fitting procedure. Since this is not a primary concern here, a model that does include $\theta_{0}$ will be used as variant of PFA and be referred to as PFA+.

\begin{equation}
P = \sigma(\sum_{c \in KC}  \gamma_{c} g_{s,c} + \rho_{c} f_{s,c} - \beta_{c})
\end{equation}

Here $\gamma$ is the learning rate of the KC for correct answers and g is the number of questions answered correctly. Consequently $\rho$ is the learning rate of the KC for incorrect answers and f is the number of questions answered incorrectly. Just as with LFA above the sign for $\beta$ was reversed compared to the original.

To compare the PFA parameter values to LFA values, the weighted average (according to the ratio g:f for each problem) of $\gamma$ and $\rho$ should be compared to $\eta$. $\theta$ cannot be compared when comparing PFA on LFA data, vice versa, $\theta$ should be compared to 0. In case of PFA+ $\theta$ values can be compared directly.


\begin{equation}
\eta: \frac{\gamma_{c} g_{c} + \rho_{c} f_{c}}{g_{c}+f_{c}}
\end{equation}

\subsection{extended Item Response Theory (eIRT)}
\label{sec:eirt}
The extended Item Response Theory model by Roijers et al \cite{eirt} is the most straightforward extension to the standard IRT model.

\begin{equation}
\label{eq:eirt}
P = \sigma(\alpha_{c} (\theta_{s,0} + \eta_{s} t_{s,c} - \beta_{c}))
\end{equation}

Similar to LFA $\theta$ is replaced by initial skill and a learning rate. Here the learning rate is taken per student though rather than per knowledge component as is done in LFA and PFA. With $\theta$ split up, it would seem that $\alpha$ obtains a slightly different meaning. For $\theta_{0}$ it still has the same discriminatory function. When looking at $\eta$ though, $\alpha$ directly impacts it as a modifier, making learning easier (>1) or more difficult (<1).

\todo{Actually I have no clue on the correct mathematical vernacular, so please nudge me to the right terms to make this more understandable/consice}
It should be noted that different parameter settings can lead to exactly the same model. E.g. all $\theta$s and $\beta$s could be increased by the same amount and the model would still be the same. To still be able to compare parameter values and variances the parameters should be normalized as follows. Take the lowest $\theta$ value to be 0 as to resolve the dependency with $\beta$ values. Take the highest $\theta$ value to be 1 as to fix the dependency between $\alpha$ and the other variables.

\subsubsection{Adapting eIRT}
eIRT as defined by Roijers et al does not incorporate multiple skill steps. In order to be trained on multi-skill data and to be similar to the other models used \ref{eq:sumeirt} will be used as a multi-skill extension of the eIRF. To distinguish this version from the original eIRT, this extended version will be denoted as seIRT. A notable difference here is that $\theta_{s,0}$ is divided by the number of KCs involved. This is because $\theta_{s,0}$ should only be added once just as in LFA/PFA, but nevertheless it should be adapted by the corresponding $\alpha_{c}$s as well.

\begin{equation}
\label{eq:sumeirt}
P = \sigma(\sum_{c \in KC} \alpha_{c}(\frac{\theta_{s,0}}{|KC|} + \eta_{s} t_{s,c} - \beta_{c}))
\end{equation}

In comparing seIRT's (or eIRT's) parameter values to those of LFA $\beta$ should be multiplied by $\alpha$. $\theta$ is to be multiplied by a weighted average of $\alpha$ (according to the ratio of KCs of the questions that the corresponding student has answered). Finally a weighted average of $\eta$ per KC should be taken according to the ratio of questions each student has answered containing that KC. seIRT can be compared to PFA by combining the steps above with the steps needed to compare PFA to LFA.


\begin{equation}
\begin{aligned}
    \beta_{c}:  &  \alpha_{c}*\beta_{c} \\
    \eta_{c}:  &  ??  
\end{aligned}
\end{equation}


\subsection{Combined Model}
The three models introduced above can all be encompassed by a more complex model.

\begin{equation}
P = \sigma(\sum_{c \in KC}\frac{\alpha_c \theta_{s,0}}{|KC|}+\eta_{s} \gamma_{c} s_{s,c} + \eta_{s}\rho_{c} f_{s,c} - \beta_{c})
\end{equation}

LFA can be obtained from this model by taking $\alpha=1$, $\eta=1$ and $\gamma=\rho$. PFA can be obtained from this model by taking $\alpha=1$, $\eta=1$ and $\theta_{0}=0$ (minus the last one for PFA+). The adapted eIRF can be obtained by taking $\gamma=\rho=\alpha$ and realizing that $\beta$ already incorporates $\alpha$.


\todo{
This section also needs the comparison stuff, but that shouldn't be too difficult. Quite similar to the ones above, just a bit more complicated
\section{Outstanding issues}
\begin{itemize}
\item Describe terms and use those consistently
\item 1-skill per item setting or multiple skill per item setting? -> depends on the next point, but it seems many are multi-dimensional
\item find good data-set(s)...
\end{itemize}
}
\bibliographystyle{alpha}   % this means that the order of references
			    % is determined by the order in which the
			    % \cite and \nocite commands appear
\bibliography{litlist}
\newpage
\appendix
\section{Glossary}
\todo{
Making a start with using terms consistently, plus I generally feel that a glossary would have helped me greatly in understanding papers etc.}
\begin{description}
  \item[Item] A problem (step) in the ITS to which a single answer can be given
  \item[Knowledge Component] A skill, piece of knowledge etc. that is associated with one or more items and in which students can have a level of competence
  \item[Question] An instance of an item
  \item[Skill] Level of an instance of a knowledge component for a particular student
\end{description}

\end{document}
