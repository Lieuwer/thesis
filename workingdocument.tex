\documentclass{scrartcl}
% \documentstyle{article}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{caption}
\begin{comment}

\end{comment}
\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{#1}}

\DeclareCaptionType{mycapequ}[][List of equations]
\captionsetup[mycapequ]{labelformat=empty}

\providecommand{\comm}[1]{{\bf[ #1 ]}}
\providecommand{\commd}[1]{\comm{D: {#1}}}

\begin{document}
\title{Master Thesis Research Proposal}
\subtitle{What can the parameters of IRT based learner models tell us?}
\author{Lieuwe Rekker}
\maketitle
\nocite{labelcombi}
\nocite{lftransfer}
\nocite{importance}
\nocite{knowledgeproblem}
\nocite{modelreview}
\nocite{eirt}
\nocite{pfa}
\nocite{ktpfa}
\nocite{skillcombi}
\nocite{lfa}
\nocite{blackart}
\nocite{hambleton}

%\listofmycapequs

\section{Introduction}
\begin{comment}
Many current intelligent tutor systems (ITS) use learner models that can give indications of to what extent a student has mastered a particular skill and even how fast students learn.  The quality of these models is generally measured by how well these models predict 'one question into the future' performance, which is what these algorithms\commd{You didn't mention any algorithms yet, just models - and even then, they maximimize the likelihood of the training data...} are made to maximize. Parameter values from these models are also inspected and stated to represent students' knowledge levels, how fast students learn, how difficult questions are etc. It would thus be wise to inspect other factors beyond the accuracy of the model to gain some idea of under what conditions they actually convey some stable real world factor or are rather part of a more black-box like model that performs well on its prediction task, but whose parameters are otherwise meaningless. \cite{knowledgeproblem} shows that this is indeed a problem: widely differing parameter settings can yield similar, (near) optimal performances. This proposal proposes research to explore if parameters of learner models hold meaning in real life.

The approach of this research is from the perspective that models resemble reality, but are necessarily not precise mirrors of it. Initially generated data will be used to see how well parameter values can be retrieved at all and to see what happens to parameters when there is a mismatch between model and 'reality' (i.e. training a model on data generated by a different model). The amount of training data will be looked at especially, since even if stable parameter values can be found, this will probably break down at some point. In the case of real data, original parameters and models are unknown. Over multiple training runs on different data-sets, variation in parameter values can be inspected. High variation here indicates that (despite possible good prediction performance of the model as a whole) the parameter does not represent anything in reality.
\end{comment}
\commd{Provide short motivation for ITS's first.} Today more and more intelligent tutor systems (ITS) are being used, both in research settings and in the world at large. An ITS is a computer program used by students to learn about different subjects and generally entails that students solve problems within the system. Data from these systems have been used to build models of how students learn.\commd{Most learner models do that indeed, but some restrict themselves to the probability of a correct answer... We don't like those, but still.} These learner models in turn have been used in ITSs to estimates the level of the student and adjusts the problems it presents to the student accordingly.\commd{Reformulate in terms of possible usuage of student models.} In research these learner models are generally evaluated by looking at some measure of how the performance predicted by the model fits the real data.\commd{Will need some cites. And, I would cute the word real.} In this research a set of learner models based on item response theory (IRT) will be discussed and it will be examined when the parameters that are fit in these models are stable.\commd{Why? motivate please}

In psychology and more notably psychometrics, how students perform on problems has long been a field of research. The activity in this field was hastened by the necessity for standardized testing. This has eventually led to the development of item response theory (IRT) (for a good overview see \cite{hambleton}). In the models stemming from IRT, students are characterized by a skill level and problems are characterized by factors such as difficulty and discrimination. Through decades of research and practical use (especially for standardized tests) IRT has gained a solid theoretical and experimental basis, which makes it a logical basis for use in ITSs in estimating the level of students. Problematic though in the application of this theory to ITS data is that learning is not taken into account.\commd{Alternative, ...is that IRT assumes the compentence of the students to be constant for the duration of the test. In ITS, which are designed to help students learn, this can obviously not be assumed.} Within a test learning is not much of an issue, but in an ITS data is collected over longer time spans and learning is actually meant to occur. \commd{ from here onwards the structure is a bit illogical. First cluster all information you want to give about IRT by itself first, then mention why you want to extend, then mention the how, then the problems that one can encounter by doing so, that a comprehensive evaluation and comparison of these models has not yet been made - and then state what your research will be, (and maybe mention the use of your research for educators and researchers)} Therefore the adaptations of IRT models used on this kind of data incorporates a learning rate.

In IRT there are established methods to see if obtained parameters are significant as well as methods that check whether its assumptions are met. This gives confidence that found parameters for students and problems are meaningful and can be used to make statements about student skill and item difficulty. In the case of the models used for ITSs, generally only model fit compared to the data (in various forms, most notably one-step-ahead prediction accuracy) is taken into account. There are some exceptions where concerns are raised on the plausibility of parameters \cite{ktpfa}, \cite{knowledgeproblem} and accuracy of parameters \cite{blackart}. Not looking further into the accuracy of these parameter estimates is at the least a missed opportunity (e.g. knowing how easy certain skills are to learn, how difficult certain items are, or how fast a particular student learns can be valuable information, even if simply to improve the ITS) and might already be used wrongly in some cases (for example making statements on the difficulty of questions, without knowing whether the fitted values can be trusted).

This research will look at the influence of how certain factors influence the accuracy of parameters estimation. When applying IRT to test results, generally no external measure is used to check the found difficulty and skill parameters against, rather internal validity is used to see if the assumptions of IRT hold \cite{hambleton}. For example, when estimating the difficulty of an item, it should not matter whether the results of low or high ability students are used to estimate the difficulty. If this is not the case, this indicates that some assumptions are not met and makes the parameter values found of questionable value. This research will take a similar approach in looking at the found parameters. The research will be split into two parts. In the first part generated data is used, so that the real parameter values are known and effects of different factors can be estimated. In the second part real data will be cut into pieces so that variance of parameters can be determined. In combination with the first part of the research it is hoped that conclusions can be drawn as to whether the parameter values found are useful and whether the data-fit measures normally looked at provide some clue to this usefulness.

\section{Related work}
\commd{Right now, this section is a laundry list: it could be improved by adding some more structure.}

In \cite{knowledgeproblem} Beck goes beyond investigating the accuracy of a model (knowledge tracing in this case) and also looks at the parameter values. The authors prime reason for concern lies in identifiability: the fact that widely differing parameter settings can lead to almost identical model outcomes. Although this paper does concern itself with the 'plausability' of parameter values it only goes so far as to nudge the parameter to values deemed plausible rather than asking the more fundamental question of whether the parameter values are useful at all.

Learning factor analysis (LFA) \cite{lfa} is a cognitive model of how students learn. It is a Rasch model\commd{is one of the IRT variants... (you have not mentioned that yet)} extended with a learning rate to model learning over time. In \cite{lfa} LFA is used in the context of a greater cognitive model of learning and skill that includes what knowledge components are linked to what items. The quality of fit (measured as the log likelihood of the entire data set given the model plus some penalty for the number of parameters) is used as a measure of quality of a specific linkage of knowledge components to items.

Performance factor analysis (PFA) is a further extension to LFA. It is introduced in \cite{pfa} as an alternative to knowledge tracing and focuses more on correctly estimating whether a student has mastered a particular knowledge component. The difference with LFA is that learning rate is dependant on whether a question is answered correctly or not. Furthermore initial student knowledge is dropped. The foremost reason for doing so is to make this model work for students of whom no data had been taken into account when fitting the model.

In \cite{ktpfa} Gong et al. also made a comparison between various knowledge tracing approaches and PFA. Whether PFA or KT performs better remains inconclusive. Upon inspecting parameter values they found that many learning rates were negative, which seemed implausible in real life. They noted that upon placing a lower bound of 0 on the learning rate, performance improved.  Additionally the authors used a pretest and correlated the performance on this test to the initial knowledge parameter from the model. In this set-up an adapted version of PFA (the same version that will be used in this research: the original doesn't use initial student knowledge) showed the highest correlation).

In \cite{blackart} Yudelson et al. show some particular factors that can negatively influence the quality of PFA models. One of the factors looked at here is model complexity: on the one hand this is done by using a more finegrained set of KCs and on the other hand by adding another parameter to the PFA model. In evaluating their results the authors did not only look at accuracy measures, but also inspected values for specific parameters. \todo{how did they get their standard deviations for the parameters? They don't seem to have split their data?} In inspecting learning parameters they also noted how the learning parameter for wrongly answered questions are often negative when initial knowledge is not included in the model. They concluded that PFA not so much models student learning in this case, but rather performs some kind of 'error tracking' in order to produce good estimates of skill despite not having any additional information on the student. They therefore prefer an adaptation of PFA that includes initial student knowledge.

In \cite{eirt} Roijers et al. extend the 2PL IRT function to include a learning rate. This new version is called extended IRT (eIRT). There are two important differences in how this model has been extended and how LFA and PFA came to be. First LFA and PFA use a multidimensional model, which means that multiple skills can be associated with a single problem. Moreover in eIRT the learning rate depends on students, while in LFA and PFA the learning rates depend on the skills. Roijers et al. perform experiments using generated data, that shows that parameters can often be retrieved from the data. Additionally they have a real life data-set based on 14 students from three different groups. With the small amount of data in their real life set, they conclude based on their previous experiment that only rule difficulty and student initial knowledge can be determined with enough accuracy. After training the model on the data the fitted initial knowledge of the three different groups confirmed their hypothesis of the ordering of average initial knowledge per group. Also the ordering of difficulty of the rules involved is indistinguishable to rankings made by experts. The authors thus showed that the difficulty and initial knowledge parameters obtained reflect the ordering of these in real life.


\section{Research Question}
\label{sec:RQ}
The question at the basis of the proposed research is "When are parameter values found in fitting IRT based models consistent and accurate". Three terms in this question need clarification: accurate, consistent and when.

Accurate and consistent are linked to the two different perspectives that will be taken in this research: looking at data generated from the used models and looking at data from ITSs. In the case of generated data it can be established if the found parameter values are accurate by comparing them to the values used to generate the data. In the case of data from ITSs no such ground truth will be used (except where this data might be at hand). Instead for these parameters consistency of the parameter values is examined. To examine this the data will be split in parts and standard deviation of the parameter values over the multiple runs will be examined. Observations on consistency using the ITS data will be compared to consistency using the generated data to examine possible causes of inconsistency. Note that this research setup means that examining whether the parameters of the model correctly operationalize the mentioned constructs is out of scope. I.e. whether for example student learning really occurs at some specific rate will not be looked at, but rather only if the model's learning rate remains consistent will be looked at.

The term 'when' in the research question has two different interpretations. The first interpretation is the different factors of which the influence will be examined. The first factor under consideration will be the amount of data. Preferably data is increased up to the point where the parameter estimates and their variances stabilize, so that a theoretical best achievable result can be established. The second factor that will be looked at is the usage of different models representing different assumptions on the domain. The models looked at are Learning Factor Analysis (LFA), Performance Factor Analysis (PFA), an adapted version of extended Item Response Theory (seIRT) and a complex model. This complex model is a generalization of the three models mentioned above. The third factor is discrepancy between the trained model and the ground truth. In the case of generated data, a model can be trained on data generated by a different model, to examine the effects. Additionally in generating data the way in which knowledge components for multi-skill items are combined (see section \ref{sec:comb}) to predict performance is varied. Since the ground truth is not known in the ITS data, the discrepancy is not known and thus cannot be examined. However it might be possible to indicate what assumptions of the models do not hold in the ITS data by looking at the experiments on generated data. Finally there are many other factors that have an influence on performance (such as parameter values and how items and KCs are linked), that will not be examined individually. For these factors domains will be used. Each ITS dataset represents a domain. In generating data, parameter values and item to KC structure and number of students, questions etc. will be taken or estimated from the dataset to represent that domain.
The second interpretation of 'when' doesn't refer to under what conditions the fit parameters are consistent and accurate, but rather if the accuracy and log likelihood of the model give an indication to the consistency and accuracy of the fitted parameters. If such a relation can be found, it can be more easily established if parameter values should be discarded as useless or might be of value.

From the research question above and the interpretation the following research sub-questions are formulated.
\begin{enumerate}
\item What is the influence of amount of data used for fitting?
\item What is the influence of using different models for fitting?
\item What is the influence of using data from different domains?
\item* What is the influence of generating data with different models?
\item* What is the relationship between the accuracy and the consistency of the fitted parameters?
\item what is the relationship between the accuracy of predictions made with the model, the likelihood of the data given the model and the accuracy/consistency of the fitted parameters?
\end{enumerate} 
Sub-questions indicated with an * only apply to the generated data case. In every sub-question influence means influence on the accuracy and/or consistency of parameter values over multiple runs.

\section{Research Priorities}
Below are my priorities for the research.

{\bf Must} Use the different IRT models on two domains. Study their performance both on artificial and real data. Examine amount of data and method in which skills are combined.

{\bf Should} Add another domain. Adapt and use methods from IRT to check if model assumptions hold.

{\bf Could} Put a normal distribution assumption on the initial knowledge component as done in \cite{blackart} and as is often done in IRT \cite{irtest}. Incorporate a difficulty per item to the model.

{\bf Won't have} Compare to a different kind of model (e.g. knowledge tracing) to see how they compare. Examine in a structured way what influence introducing noise to the knowledge component to items linkage has on parameter estimates.

\section{Method}


\subsection{Model Performance}

\label{sec:perf}
The most common performance measure used in the context of ITS learning models is some measure of accuracy of model predictions on next-item student performance. Although this research looks into the values of model parameters and is less concerned with other measures of performance an accuracy measure will still be used. The main rationale behind is is that the accuracy of a model may hold some relationship to how well the parameters are matched. This would be especially useful as accuracy measures are relatively easy to obtain and are often already looked at in ITSs. \todo{refer back to the rq}

The specific accuracy measure used for this research will be A' \cite{modelreview}. For this measure two items are represented to the model, one which was answered correctly and one which was answered incorrectly. The model is used to determine which is which. The advantage of this method is that "values of A' are statistically comparable across models and data sets" \cite{modelreview}.

Log likelihood of the data given a model is another measure that plays a large role in this specific context as this is what is maximized in fitting the model. Although the log likelihood value might not be very informative, it does give an indication of how well the model fits the data-set and in combination with the measured accuracy one can get an idea to what extent overfitting occurs.

In this research the performance of interest is how meaningful the parameter values of the used models are. This will not be done by looking at the external validity of the values found through experimentation with students, but rather by looking at the stability of the values found from real data and the deviance from the true values in the case of generated data.

In the case of generated data, multiple runs will be done so that for each parameter an average and a deviation can be found. These will then be used to compare the retrieved parameter values to the values used to generate the data. From these results it is hoped that some conclusion can be drawn whether parameter values found could be useful at all.
\begin{comment}
this probably needs to go. Not sure what I've been thinking here.
The models examined have some differences in what parameters are used exactly. In each models' paragraph it will therefore be described how the values (and variance) of its parameters can be compared to the values (and variance) of parameters of the models discussed before it.
\end{comment}
In the case of real data, the dataset will be split up in continuous consecutive pieces to obtain multiple training sets. For all parameters except initial knowledge ($\theta_{s,0}$) a variance over the different 'runs' can be established. The found values and variances in this case cannot be compared to some baseline, but should be used to see if values are distinguishable from neighboring values (if not the values found mean quite little) and variances found here might be compared to those found in the generated data experiments to see if some similarity (or lack thereof) is seen. In the case of initial knowledge, it will instead be checked to see to what extend the next initial knowledge estimate deviates from what would have been predicted by the model fitted on the preceding data.

Additionally a method that from IRT \cite{hambleton} will be used. After initially fitting the data will be split in two along skills, such that one set contains all the easiest skills and one will contain all the most difficult skills. Then the fitting will be done again on both new data-sets. If the assumptions of the model were correct the parameters for students found should be roughly the same from both sets. This procedure will be repeated, but then making a split on student initial skill and looking at the skill parameters. \todo{it seems more approaches like this can be adapted. Should more justification be given?}

Although the primary way in which model parameters will be examined is described above, alternative methods will be used where applicable. Some data-sets contain additional information that can be used to more directly draw conclusions on whether found parameter values are meaningful. Two examples from the related research section are \cite{eirt} where expert opinions and an indication of what groups have higher initial knowledge were used and \cite{ktpfa} where a pre-test was used as an indicator of initial knowledge.

\subsection{Data Folding}
\todo{maybe shouldn't be a different subsection? integrate with above, or make some subsubsections?}
Doing multiple training runs for every model means that the real data will be split into parts that possibly partially overlap. Some consideration needs to be spend on balancing the number of runs, the possible amount of data per run and overlap of data between runs and what the consequences will be (e.g. variance of parameter values trained on large data sets may be less accurate or lower due to using less runs or overlapping data respectively)

In increasing the data the number of students and the number of problems and knowledge components are held constant. This is to ensure that the number of parameters that are fitted will remain the same over the different test runs. Parameter values will be obtained over the entire folds, while A' values will be determined by testing against data of the next fold.

\subsection{Domains}
\label{sec:domain}
In generating the data there are many other factors that might play a role in the performance of the models. Among those factors are the values that the parameters will be given, the distribution of knowledge components over the items, the ratio of students to items etc. In order to make the generated data experiments resemble reality, the experiments (or at least some) on real data should be performed first to use values from those experiments for the generated data. In the same way structural and statistical elements (such as linkage of knowledge components to items, distribution of question answered per student etc.) will be obtained from the data. This will be done for every data set used. These settings each represent a different domain.

\subsection{Experiments}
In synopsis the research will contain the following experiments/steps
\begin{enumerate}
\item Generated data
\begin{enumerate}
\item Do preliminary runs with every model on every domain to obtain 'domain parameter values' (see \ref{sec:domain})
\item Generate data using every combination of model and domain. Train each model on each dataset using increasing amounts of data. Compare results (see \ref{sec:perf})
\item Generate data using every combination of model and domain and the two alternative ways of combining knowledge components (see \ref{sec:comb})
\end{enumerate}
\item Train each model on the real data sets, also using increasing amounts of data and compare results (variances, log likelihoods and accuracies) to each other and the runs on generated data (see \ref{sec:perf})
\end{enumerate}


\section{Models}
All models used in this research are based on item response theory and use a version the 2 parameter item response function (IRF) \commd{No they don't, the first couple you mention use the 1 parameter IRT, also called Rasch, model}. The basis of this function is the logistic ogive function \ref{eq:logistic}. The most basic 2 parameter IRF is shown in \ref{eq:irt}. Here i stands for item and s stands for student. $\alpha$ is the discriminatory power of the item i.e. how much knowledge matters in how well a student will do on that item. $\beta$ is the difficulty of the item i.e. how high a students skill should be to achieve a P of .5. $\theta$ is the skill of a student, indicating how well the student has mastered a knowledge component. The value of the function is interpreted as the probability that the student will answer a question correctly. This is the same interpretation that is used throughout all models in this research.

\begin{equation}
\label{eq:logistic}
\sigma(x) = \frac{1}{1+e^-x}
\end{equation}

\begin{equation}
\label{eq:irt}
P = \sigma(\alpha_{i} (\theta_{s} - \beta_{i}))
\end{equation}

All models used in this research employ an important extension: it incorporates learning by students. Skill, $\theta$ is split up into an initial part and a learning rate, so that each time a question is answered the skill of the student increases. In the ITS there are many different knowledge components and thus the parameters are fit per knowledge component. Depending on the data-set used an item can have multiple components associated with them, creating the necessity to combine multiple item response functions.

\todo{Say something about the fitting procedure for each seperate model.}\commd{what, here? or do you mean the alternate ML optimization of student and item parameters? }

\commd{List the assumptions each model makes. Either in each subsection, or in a separate subsection}

\subsection{Learning Factor Analysis (LFA)}
\label{sec:LFA}


LFA uses a simplified version of the IRF, but extents it by introducing a learning rate as discussed in the introduction and by allowing multiple knowledge components to be associated with a single item. The combination of KCs is made by summing the learned part of knowledge and the difficulty of the KC for every KC that is linked to the item.

\begin{equation}
P = \sigma(\theta_{s,0} + \sum_{c \in KC}  \eta_{c} t_{s,c} - \beta_{c})
\end{equation}
\commd{So the start competence is the same for every knowledge component/skill, but the learning speed can vary per KC/skill... that is an assumption that is explicitly rejected in the eIRT paper.}

The simplification of the model is done by dropping $\alpha$ (this model is called the Rasch model). The splitting of $\theta$ leads to the introduction of an initial skill $\theta_{0}$ defined per student, a learning rate $\eta$ defined per KC (i.e. the KC determines how fast or slow learning occurs) and a number of times that a student has seen items belonging to this particular knowledge component $t_{s,c}$. Please note that in the the original LFA $\beta$ is added. It is subtracted here to maintain similarity to the original IRF and ensure uniformity with the other models used. This will have no other effect than that the signs for $\beta$ will be reversed.

When looking at a data set where only a single knowledge component is linked to every item, $\beta$ and $\theta_{0}$ are not independent: we can raise both by any amount. To solve this issue, the average value of $\theta_{0}$ will be set to 0.
\subsubsection{Combining knowledge components}
\label{sec:comb}
Something more can be said about the way KCs are combined. In \cite{skillcombi} Cen et al. show that in practice there is no difference in performance between an additive model (as used here) or a conjunctive model (where probabilities of individual KCs are multiplied). Nevertheless the authors already mention that this is probably the case because for most KCs $\beta < \eta_{c} t_{s,c}$, meaning that adding KCs does decrease the chance of answering the question correctly as would be expected. In their paper they already propose using a data set where ($\beta > \eta_{c} t_{s,c}$) to see if this is indeed why this way of combining KCs works well in practice.

The experiment proposed above will be put to the test here. Fitting a conjunctive model is hard in practice, but generating data using one is rather straightforward. Whether a real life data set contains many questions where skills are such that $\beta < \eta_{c} t_{s,c}$ cannot be said at this point. Even if this is not the case though, it can be argued that the parameter values can be skewed slightly to ensure that this occurs. The rationale behind is, is that the fitted values obtained from the real data may be skewed towards $\beta < \eta_{c} t_{s,c}$ due to a additive model being used in the fitting process. It would then be expected though that the retrieved parameters using these values would be skewed towards $\beta < \eta_{c} t_{s,c}$ again.

\subsection{Performance Factor Analysis (PFA)}
PFA is a direct extension of LFA. In PFA separate learning rates are used for questions answered correctly and questions answered incorrectly. Additionally $\theta_{0}$ is dropped. As put forward in \cite{pfa} $\theta_{0}$ is dropped because this extension is made mostly to make this model more useful in ITSs. Leaving out any student specific parameter makes the model more easily applicable to students not used in the fitting procedure. As noted in both \cite{ktpfa} and \cite{blackart}, leaving out $\theta_{s,0}$ makes parameter estimates worse. Since prediction for students who were not part of the fitting procedure is not a primary concern here, a model that does include $\theta_{s,0}$ (as done in \cite{ktpfa} and \cite{blackart}) will be used instead of PFA and will be referred to as PFA+.

\begin{equation}
P = \sigma(\theta_{s,0} \sum_{c \in KC}  \gamma_{c} g_{s,c} + \rho_{c} f_{s,c} - \beta_{c})
\end{equation}
\commd{Is there a minus sign missing here? same assumption as the last, but two different learning rates. }

Here $\gamma$ is the learning rate of the KC for correct answers and g is the number of questions answered correctly. Consequently $\rho$ is the learning rate of the KC for incorrect answers and f is the number of questions answered incorrectly. Just as with LFA above the sign for $\beta$ was reversed compared to the original.

\begin{comment}
To compare the PFA parameter values to LFA values, the weighted average (according to the ratio g:f for each problem) of $\gamma$ and $\rho$ should be compared to $\eta$. $\theta$ cannot be compared when comparing PFA on LFA data, vice versa, $\theta$ should be compared to 0. In case of PFA+ $\theta$ values can be compared directly.

\begin{mycapequ}
\begin{equation}
\eta_{c}: \frac{\gamma_{c} g_{c} + \rho_{c} f_{c}}{g_{c}+f_{c}}
\end{equation}
\caption{Comparison of LFA parameters to PFA parameters}
\end{mycapequ}
\end{comment}

\subsection{extended Item Response Theory (eIRT)}
\label{sec:eirt}
The extended Item Response Theory model by Roijers et al \cite{eirt} is the most straightforward extension to the standard IRT model.

\begin{equation}
\label{eq:eirt}
P = \sigma(\alpha_{c} (\theta_{s,0} + \eta_{s} t_{s,c} - \beta_{c}))
\end{equation}
\commd{Only one competence and learning speed.}

Similar to LFA $\theta$ is replaced by initial skill and a learning rate. Here the learning rate is taken per student though rather than per knowledge component as is done in LFA and PFA. With $\theta$ split up, it would seem that $\alpha$ obtains a slightly different meaning. For $\theta_{0}$ it still has the same discriminatory function. When looking at $\eta$ though, $\alpha$ directly impacts it as a modifier, making learning easier (>1) or more difficult (<1).

\todo{Actually I have no clue on the correct mathematical vernacular, so please nudge me to the right terms to make this more understandable/consice}
It should be noted that different parameter settings can lead to exactly the same model. E.g. all $\theta$s and $\beta$s could be increased by the same amount and the model would still be the same. To still be able to compare parameter values and variances the parameters should be normalized as follows. The average of $\theta{s,0}$ will be set to zero as to resolve the dependency with $\beta$ values. The standard deviation of $\theta$ will be set to 1 as to fix the dependency between $\alpha$ and the other parameters.

\subsubsection{Adapting eIRT}
eIRT as defined by Roijers et al does not incorporate multiple skill steps. In order to be trained on multi-skill data and to be similar to the other models \ref{eq:sumeirt} will be used as a multi-skill extension of the eIRF. To distinguish this version from the original eIRT, this extended version will be denoted as seIRT. A notable difference here is that $\theta_{s,0}$ is divided by the number of KCs involved. This is because $\theta_{s,0}$ should only be added once just as in LFA/PFA, but nevertheless it should be modified by the corresponding $\alpha_{c}$s as well.

\begin{equation}
\label{eq:sumeirt}
P = \sigma(\sum_{c \in KC} \alpha_{c}(\frac{\theta_{s,0}}{|KC|} + \eta_{s} t_{s,c} - \beta_{c}))
\end{equation}

Please note that in the multidimensional case the dependency between $\theta{s,0}$ and $\beta$ is most likely no longer there. The dependency between $\alpha$ and the other parameters still exists though and should still be resolved by fixing the standard deviation of $\theta_{s,0}$ to 1.

\begin{comment}
In comparing seIRT's (or eIRT's) parameter values to those of LFA $\beta$ should be multiplied by $\alpha$. $\theta$ is to be multiplied by a weighted average of $\alpha$ (according to the ratio of KCs of the questions that the corresponding student has answered). Finally a weighted average of $\eta$ per KC should be taken according to the ratio of questions each student has answered containing that KC. seIRT can be compared to PFA by combining the steps above with the steps needed to compare PFA to LFA.

\commd{Let's discuss this: $\eta_s$ is constant per student, and alpha should be inside the sum if you want ot avg it. }


\begin{mycapequ}[!ht]
\begin{equation}
\begin{aligned}
    \beta_{c}:  &  \alpha_{c}*\beta_{c} \\
    \eta_{c}:  &  \alpha_{c}*\frac{\sum_{s \in S} \eta_{s}*t_{s,c}} {|t_{c}|} \\
    \theta_{s,0}: & \theta_{s,0}*\frac {\sum_{c \in KC} \alpha_{c}*t_{s,c}} {|t_{s}|}
\end{aligned}
\end{equation}
\caption{Comparison of LFA parameters to seIRT parameters}
\end{mycapequ}
\end{comment}

\subsection{Combined Model}
The three models introduced above can all be encompassed by a more complex model.

\begin{equation}
P = \sigma(\sum_{c \in KC}\frac{\alpha_c \theta_{s,0}}{|KC|}+\eta_{s} \gamma_{c} g_{s,c} + \eta_{s}\rho_{c} f_{s,c} - \beta_{c})
\end{equation}

LFA can be obtained from this model by taking $\alpha=1$, $\eta=1$ and $\gamma=\rho$. PFA can be obtained from this model by taking $\alpha=1$, $\eta=1$ and $\theta_{0}=0$ (minus the last one for PFA+). The adapted eIRF can be obtained by taking $\gamma=\rho=\alpha$ and realizing that $\beta$ already incorporates $\alpha$.

\begin{comment}
\begin{mycapequ}
\begin{equation}
\begin{aligned}
    \gamma_{c}:  & \gamma_{c}*\frac{\sum_{s \in S} \eta_{s}*g_{s,c}} {|g_{c}|} \\
    \rho_{c}:  & \rho_{c}*\frac{\sum_{s \in S} \eta_{s}*f_{s,c}} {|f_{c}|} \\
    \theta_{s,0}: & \theta_{s,0}*\frac {\sum_{c \in KC} \alpha_{c}*t_{s,c}} {|t_{s}|}
\end{aligned}
\end{equation}
\caption{Comparison of PFA parameters to full model parameters}
\end{mycapequ}

\begin{mycapequ}
\begin{equation}
\begin{aligned}
    \eta_{s}: \eta_{s}* \frac{\sum_{c \in KC} \frac{\gamma_{c} g_{c} +  f_{c}}{\alpha_{c}}}{g_{c}+f_{c}} \\
    \beta_{c}:  & \rho_{c}*\frac{\sum_{s \in S} \eta_{s}*f_{s,c}} {|f_{c}|}
\end{aligned}
\end{equation}
\caption{Comparison of seIRT parameters to full model parameters}
\end{mycapequ}
\end{comment}


\todo{
\section{Outstanding issues}
\begin{itemize}
\item In evaluating will accuracy in general be looked at, or rather will a more ordinal scale approach be taken
\item Describe terms and use those consistently!
\item get a better idea how A' should be applied in this context
\item related work is a bit out of place as it is not very intelligible without information given later. Maybe move it to the end, or rather integrate it with other stuff?
\item 1-skill per item setting or multiple skill per item setting? -> depends on the next point, but it seems many are multi-dimensional
\item find good data-set(s)...
\end{itemize}
}
\bibliographystyle{alpha}   % this means that the order of references
			    % is determined by the order in which the
			    % \cite and \nocite commands appear
\bibliography{litlist}
\newpage
\appendix
\section{Glossary}
\todo{
Making a start with using terms consistently, plus I generally feel that a glossary would have helped me greatly in understanding papers etc.}
\begin{description}
  \item[Item] A problem (step) in the ITS to which a single answer can be given
  \item[Knowledge Component] A skill, piece of knowledge etc. that is associated with one or more items and in which students can have a level of competence
  \item[Question] An instance of an item
  \item[Skill] Level of an instance of a knowledge component for a particular student
\end{description}

\end{document}
