\documentclass{scrartcl}
% \documentstyle{article}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{subfig}
\begin{comment}

\end{comment}
\usepackage{xcolor}
\newcommand\todo[1]{\textit{\textcolor{red}{#1}}}

\DeclareCaptionType{mycapequ}[][List of equations]
\captionsetup[mycapequ]{labelformat=empty}

\providecommand{\comm}[1]{{\bf[ #1 ]}}
\providecommand{\commd}[1]{\comm{D: {#1}}}

\begin{document} 
\title{Item Reponse Theory in Intelligent Tutoring Systems}
\subtitle{Can IRT-based Learner Models used in an IRT Goals?}

\author{Lieuwe Rekker}
\maketitle
\nocite{labelcombi}
\nocite{lftransfer}
\nocite{importance}
\nocite{knowledgeproblem}
\nocite{modelreview}
\nocite{eirt}
\nocite{pfa}
\nocite{ktpfa}
\nocite{skillcombi}
\nocite{lfa}
\nocite{blackart}
\nocite{hambleton}
\nocite{bridge}
\nocite{ct}
\nocite{algebra}

%\listofmycapequs

\section{Introduction}
\begin{comment}
Many current intelligent tutor systems (ITS) use learner models that can give indications of to what extent a student has mastered a particular skill and even how fast students learn.  The quality of these models is generally measured by how well these models predict 'one question into the future' performance, which is what these algorithms\commd{You didn't mention any algorithms yet, just models - and even then, they maximimize the likelihood of the training data...} are made to maximize. Parameter values from these models are also inspected and stated to represent students' knowledge levels, how fast students learn, how difficult questions are etc. It would thus be wise to inspect other factors beyond the accuracy of the model to gain some idea of under what conditions they actually convey some stable real world factor or are rather part of a more black-box like model that performs well on its prediction task, but whose parameters are otherwise meaningless. \cite{knowledgeproblem} shows that this is indeed a problem: widely differing parameter settings can yield similar, (near) optimal performances. This proposal proposes research to explore if parameters of learner models hold meaning in real life.

The approach of this research is from the perspective that models resemble reality, but are necessarily not precise mirrors of it. Initially generated data is used to see how well parameter values can be retrieved at all and to see what happens to parameters when there is a mismatch between model and 'reality' (i.e. training a model on data generated by a different model). The amount of training data is looked at especially, since even if stable parameter values can be found, this will probably break down at some point. In the case of real data, original parameters and models are unknown. Over multiple training runs on different data-sets, variation in parameter values can be inspected. High variation here indicates that (despite possible good prediction performance of the model as a whole) the parameter does not represent anything in reality.
\end{comment}
\begin{comment}
\commd{Provide short motivation for ITS's first.} Today more and more intelligent tutor systems (ITS) are being used, both in research settings and in the world at large. An ITS is a computer program used by students to learn about different subjects and generally entails that students solve problems within the system. Data from these systems have been used to build models of how students learn.\commd{Most learner models do that indeed, but some restrict themselves to the probability of a correct answer... We don't like those, but still.} These learner models in turn have been used in ITSs to estimates the level of the student and adjusts the problems it presents to the student accordingly.\commd{Reformulate in terms of possible usuage of student models.} In research these learner models are generally evaluated by looking at some measure of how the performance predicted by the model fits the real data.\commd{Will need some cites. And, I would cute the word real.} In this research a set of learner models based on item response theory (IRT) will be discussed and it will be examined when the parameters that are fit in these models are stable.\commd{Why? motivate please}

In psychology and more notably psychometrics, how students perform on problems has long been a field of research. The activity in this field was hastened by the necessity for standardized testing. This has eventually led to the development of item response theory (IRT) (for a good overview see \cite{hambleton}). In the models stemming from IRT, students are characterized by a skill level and problems are characterized by factors such as difficulty and discrimination. Through decades of research and practical use (especially for standardized tests) IRT has gained a solid theoretical and experimental basis, which makes it a logical basis for use in ITSs in estimating the level of students. Problematic though in the application of this theory to ITS data is that learning is not taken into account.\commd{Alternative, ...is that IRT assumes the compentence of the students to be constant for the duration of the test. In ITS, which are designed to help students learn, this can obviously not be assumed.} Within a test learning is not much of an issue, but in an ITS data is collected over longer time spans and learning is actually meant to occur. \commd{ from here onwards the structure is a bit illogical. First cluster all information you want to give about IRT by itself first, then mention why you want to extend, then mention the how, then the problems that one can encounter by doing so, that a comprehensive evaluation and comparison of these models has not yet been made - and then state what your research will be, (and maybe mention the use of your research for educators and researchers)} Therefore the adaptations of IRT models used on this kind of data incorporates a learning rate.

In IRT there are established methods to see if obtained parameters are significant as well as methods that check whether its assumptions are met. This gives confidence that found parameters for students and problems are meaningful and can be used to make statements about student skill and item difficulty. In the case of the models used for ITSs, generally only model fit compared to the data (in various forms, most notably one-step-ahead prediction accuracy) is taken into account. There are some exceptions where concerns are raised on the plausibility of parameters \cite{ktpfa}, \cite{knowledgeproblem} and accuracy of parameters \cite{blackart}. Not looking further into the accuracy of these parameter estimates is at the least a missed opportunity (e.g. knowing how easy certain skills are to learn, how difficult certain items are, or how fast a particular student learns can be valuable information, even if simply to improve the ITS) and might already be used wrongly in some cases (for example making statements on the difficulty of questions, without knowing whether the fitted values can be trusted).

This research will look at the influence of how certain factors influence the accuracy of parameters estimation. When applying IRT to test results, generally no external measure is used to check the found difficulty and skill parameters against, rather internal validity is used to see if the assumptions of IRT hold \cite{hambleton}. For example, when estimating the difficulty of an item, it should not matter whether the results of low or high ability students are used to estimate the difficulty. If this is not the case, this indicates that some assumptions are not met and makes the parameter values found of questionable value. This research will take a similar approach in looking at the found parameters. The research will be split into two parts. In the first part generated data is used, so that the real parameter values are known and effects of different factors can be estimated. In the second part real data will be cut into pieces so that variance of parameters can be determined. In combination with the first part of the research it is hoped that conclusions can be drawn as to whether the parameter values found are useful and whether the data-fit measures normally looked at provide some clue to this usefulness.
\end{comment}
\section{Models}

\subsection{IRT Models}
For every question asked an IRT model uses the student $s$ and the item $i$ as input and returns a probability $P$ that the question is answered correctly. The function at the basis of providing this probability is the logistic ogive function (see formula \ref{eq:logistic}). Each student has exactly one parameter associated with them which represents the ability of the student and is here named skill and indicated by $\theta$. This parameter is generally the parameter of highest interest in tests as it can be used to order the level of skill of the different students. The number of parameters associated with items and how all these are combined into $x$ differs per particular model and is discussed in the following paragraphs.
\begin{equation}
\label{eq:logistic}
\sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}

\subsubsection{1PL or Rasch Model}
\label{sec:1PL}
The 1PL model, also known as the Rasch model has one parameter ($b$) per item, which stands for the difficulty of the item. The entire formula for a question's probability of a correct answer is $P(s,i) = \sigma(\theta_{s} - b_{i})$. This means that when the skill of the student and the difficulty of the item are on a par, the student has a probability of .5 to answer the question correctly. Note that there exists an indeterminacy issue with this model: one can keep the same model while changing the parameter values by increasing or decreasing all $\theta$ and $b$ by the same amount. This problem is generally solved by setting the average $\theta$ to 0.

\subsubsection{2PL Model}
The 2PL model expands the 1PL model with the parameter $a$ which is called the discrimination of the item. The 2PL then looks like this: $P(s,i) = \sigma(a_{i} (\theta_{s} - b_{i}))$ The term discrimination comes from the fact that a high discrimination causes $P$ to change quickly when $|(\theta_{s} - b_{i})|$ is small and thus the performance of students who are close in skill can be more easily distinguished. The flip-side here is that when $|(\theta_{s} - b_{i})|$ isn't small, $P$ will more quickly drop to 0 or rise to 1, concealing any difference between skill levels at those levels. Note that for this model not only can $\theta$ and $b$ be increased or decreased by the same amount, but all $a$ can be scaled up or down as long as all $\theta$ and $b$ are scaled down or up respectively by the same factor. This problem is generally solved by setting the average $\theta$ to 0 and its variance to 1. 

\subsubsection{3PL Model}
The final IRT model discussed here is the 3PL model which adds a chance parameter $c$. This model takes into account that on occasion the student could answer a question correctly by taking a (educated) guess. This phenomenon is of course most prevalent in multiple choice tests where the chances of correctly guessing the answer are relatively high. The model effectively changes the lowest probability to the level of $c$ and the space between $c$ and $1$ is rescaled accordingly leading to formula \ref{eq:3PL}. This model suffers from the same identifiability issues as the 2PL model. In this thesis this model is left out of consideration, as none of the learning models are based on it and none of the data is multiple choice and will thus not be mentioned again. 

\begin{equation}
\label{eq:3PL}
P(s,i)= c_{i} + \frac{1-c_{i}}{1+e^{-a_{i}(\theta_{s} - b_{i})}}
\end{equation}

\subsubsection{Fitting the Models}
The above paragraphs have discussed what the parameters are taken to represent and how they are used in the model. None of these parameters are directly observable (i.e. they are latent) and thus are not directly obtained from observations. Instead the observed answers to the questions asked and the model probabilities for those answers are used. The parameters are given those values at which the likelihood that the observed answers arise from the model is maximized. The likelihood of a single question is the probability that the correctness of the answer is seen according to the model. Thus if the question is answered correctly, the likelihood for that answer is $P$, while if the question is answered incorrectly the likelihood is $1-P$. By taking the product of the likelihoods of every data point the likelihood of the entire dataset is determined, which gives formula \ref{eq:likely}.

\begin{equation}
\label{eq:likely}
L=\prod_{d \in D} P_{d}^{t_d}  (1- P_{d})^{1-t_d}
\end{equation}

In the 1PL model $x$ is linear in the parameters, which makes maximizing the likelihood quite straightforward: logistic regression can be directly applied to this problem. In the 2PL $x$ is bi-linear and thus logistic regression can not be applied directly. Instead values for student parameters are fixed (making $x$ linear again), logistic regression is applied, the found item parameter values are then kept fixed to find the values for the student parameters. This procedure is repeated until the likelihood of the data is (nearly) the same in consecutive runs of logistic regression. For more detailed information on logistic regression and how it is used here, please refer to appendix \ref{app:math}.

Note that this model can go haywire if a student answers all his questions correctly or incorrectly (the students ability will run to plus or minus infinitely respectively) or if an item is always answered correctly  or incorrectly. To prevent this issue these are removed from the data before fitting.

\subsubsection{Information Function}
\label{sec:inherent}
All IRT models are stochastic in nature: even when the probability of a correct answer is 90\%, an incorrect answer is still expected in 10\% of the cases. This means that even if data would be noiselessly generated by the model, some variation will be present if we generate a dataset multiple times for the same set of questions. This also means that even though the parameters used to generate this data are exactly the same, the parameters found in the fitting process will be different for each generated dataset. This is some variance in the found parameters that will last even if the data is noiseless.  

To justify conclusions drawn from application of IRT, these variance are approximated by information functions. Baker \cite{basicbaker} describes an item information function as a function based on a set of items (with known parameters) that returns the variance in skill for a student that would be found if a student would answer this set of items over and over again, while every time forgetting he has seen these items. The independent variable in this item information function is the skill of the student. The reason becomes clear from a small though experiment. If a students skill is so low that he would probably give only wrong answers to the set of items, the estimates will be very inaccurate: the value of his skill could be a large negative value, but it might just as well be twice as small. Students with about average skill probably give a correct answer to about half the items, leaving far less uncertainty about their skill.

An equivalent information function can also be made for a group of students (again with known parameters) where, given parameters for an item, the variances for its parameters is given. An information function can even be defined for a set of questions where student and item parameters are obtained concurrently. Note that in all these cases only the data (the questions) are important and not the labels (the answers). The answers do influence the found variances indirectly through their influence on the fitted value of the parameters.



\subsubsection{Assessment of fit}
The issue with IRT is that the parameters which are of interest are not directly observable. It might be possible through the use of experts to get an indication of what values, but this would at least be costly and still difficult to do. There is thus no direct way to check the values of the parameters. In \cite{hambleton} Hambleton puts forward three types of evidence to inspect model fit:

\begin{quote}"[J]udgements about the fit of the model to the test data be based on three types of evidence: 1. Validity of the assumptions of the model for the test data 2. Extend to which the expected properties of the model (e.g., invariance of item and ability parameters) are obtained 3. Accuracy of model predictions using real and, if appropriate, simulated test data."[p.55]
\end{quote}

The third point is most straightforward: if the predictions by the model for questions that were not used in fitting are inaccurate, the model is probably not a good fit for the data. The most important assumption the first point refers to is that of unidimensionality: the skill represented by $\theta$ should be the only skill of importance in answering the items. A good example is when some questions on a math question use difficult wording, making a high skill in math insufficient for correctly answering this item, while a student with mediocre math skill, but good language skills might answer the questions correctly. The second point and especially the mentioned example of invariance is of major importance for IRT. In Hambleton's own words: "The property of invariance of item and ability parameters is the cornerstone of IRT". Invariance means that the values obtained for items are the same whether they are fitted on the data obtained from one group of students or another group of students and the same for the parameter values of students. To test for invariance Hambleton proposes to split the students/items in two and see if the parameters of the items/students fit on the two different sets resemble each other. The method he uses for this is to plot them against each other and see if this plot produces a straight line. The property of invariance should go even further though, and he proposes to split the students/items such that the highest skilled/most difficult ones are in the same group and repeat the same procedure. He concedes that some more scatter is expected at the low and high ends of the parameter scales due to the higher inherent variance discussed in the previous section.




\subsection{IRT based learner models}
Although the models used in this research are based on the 1PL and 2PL models, there are also some important differences. The first and most obvious is that they incorporate learning, as students' skill level is expected to increase as they answer questions. To represent this, skill in the model, $\theta$, is split up in an initial part and a learned part, such that each time they answer a question the students skill increases. 
Another major difference is that the assumption that a single skill is measured is dropped. In these models a single item be associated with multiple skills. 

Additionally to this a subtle but major change was made to the item parameters in the model. Instead of assigning parameter values to items, value parameters are assigned to the dimensions of skill, named knowledge components (KC). Thus in these models an item is associated to one or more Knowledge components that have their own difficulty level etc. The item itself no longer has parameters, only the KCs it is associated with do. From a data perspective this makes sense as the number of knowledge components is smaller than the number of items, reducing the number of parameters that need to be fit.

\subsubsection{Additive Factor Model (AFM)}
\label{sec:AFM}


The AFM has the 1PL model as its basis, but extents it by introducing a learning rate as discussed in the introduction and by allowing multiple knowledge components to be associated with a single item. The combination of KCs is made by summing the learned part of knowledge and the difficulty of the KC for every KC that is linked to the item.

\begin{equation}
\label{eq:afm}
P = \sigma(\theta_{s,0} + \sum_{c \in KC}  \eta_{c} t_{s,c} - \beta_{c})
\end{equation}

The splitting of $\theta$ leads to the introduction of an initial skill $\theta_{0}$ defined per student, a learning rate $\eta$ defined per KC (i.e. the KC determines how fast or slow learning occurs) and a number of times that a student has seen items associated with this particular knowledge component $t_{s,c}$. Please note that in the the original AFM $\beta$ is added. It is subtracted here to maintain similarity to the original IRT models and ensure uniformity with the other models used. This has no other effect than that the signs for $\beta$ are reversed.

The indeterminacy that can occur in the 1PL model (discussed in section \ref{sec:1PL}) is absent here when items are associated with a different number of knowledge components (which is expected). Raising $\beta$ and $\theta_{0}$ by the same amount will affect items with a different number of associated KCs differently leading to different Ps. 



\subsubsection{Performance Factor Analysis (PFA)}
The PFA model is a direct extension of the AFM. In the PFA model separate learning rates are used for questions answered correctly and questions answered incorrectly. Additionally $\theta_{0}$ is dropped. As put forward in \cite{pfa} $\theta_{0}$ is dropped because this extension is made mostly to make this model more useful in ITSs. Leaving out any student specific parameter makes the model more easily applicable to students not used in the fitting procedure. As noted in both \cite{ktpfa} and \cite{blackart}, leaving out $\theta_{s,0}$ makes parameter estimates worse. Since prediction for students who were not part of the fitting procedure is not a concern here, a model that does include $\theta_{s,0}$ (as done in \cite{ktpfa} and \cite{blackart}) is used instead of PFA and will be referred to as PFA+.

\begin{equation}
\label{eq:pfa}
P = \sigma(\theta_{s,0} \sum_{c \in KC}  \gamma_{c} g_{s,c} + \rho_{c} f_{s,c} - \beta_{c})
\end{equation}
\commd{Is there a minus sign missing here? same assumption as the last, but two different learning rates. }

Here $\gamma$ is the learning rate of the KC for correct answers and g is the number of questions answered correctly. Consequently $\rho$ is the learning rate of the KC for incorrect answers and f is the number of questions answered incorrectly. Just as with the AFM above the sign for $\beta$ is reversed compared to the original representation of the model.

\subsubsection{extended Item Response Theory (eIRT)}
\label{sec:eirt}
The extended Item Response Theory model by Roijers et al \cite{eirt} is the most straightforward extension to a standard IRT model and is different from the previous two models in that it is unidimensional. It is an extension of the 2PL model and splits $\theta$ into an initial skill and a learned part. 

\begin{equation}
\label{eq:eirt}
P = \sigma(\alpha_{c} (\theta_{s,0} + \eta_{s} t_{s,c} - \beta_{c}))
\end{equation}
\commd{Only one competence and learning speed.}

Although the incorporation of a learning rate is similar to the above two models, there is a major difference: here the learning rate is taken per student rather than per KC. With $\theta$ split up, it would seem that $\alpha$ obtains a slightly different meaning. For $\theta_{0}$ it still has the same discriminatory function. When looking at $\eta$ though, $\alpha$ directly impacts it as a modifier, making learning easier (>1) or more difficult (<1) for that item.

As this is a unidimensional item the problem of indeterminacy is also at play here exactly as it was in the 2PL model. This problem could be remedied in the same way as in the 2PL model.

eIRT as defined by Roijers et al does not incorporate multiple skills (KCs) per item. In order to be trained on multi-skill data and to be similar to the other models \ref{eq:sumeirt} will be used as a multi-skill extension of the eIRF. To distinguish this version from the original eIRT, this extended version will be denoted as seIRT. A notable difference here is that $\theta_{s,0}$ is divided by the number of KCs involved. This is because $\theta_{s,0}$ should only be added once just as in LFA/PFA, but nevertheless it should be modified by the corresponding $\alpha_{c}$s as well.

\begin{equation}
\label{eq:sumeirt}
P = \sigma(\sum_{c \in KC} \alpha_{c}(\frac{\theta_{s,0}}{|KC|} + \eta_{s} t_{s,c} - \beta_{c}))
\end{equation}

Please note that in the multidimensional case the dependency between $\theta{s,0}$ and $\beta$ is most likely no longer there as described for the AFM. The dependency between $\alpha$ and the other parameters still exists though and should still be resolved by fixing the standard deviation of $\theta_{s,0}$ to 1 and normalizing the other parameters accordingly.

\begin{comment}


\subsubsection{Combining knowledge components}
\label{sec:comb}
Something more can be said about the way KCs are combined. In \cite{skillcombi} Cen et al. show that in practice there is no difference in performance between an additive model (as used here) or a conjunctive model (where probabilities of individual KCs are multiplied). Nevertheless the authors already mention that this is probably the case because for most KCs $\beta < \eta_{c} t_{s,c}$, meaning that adding KCs does decrease the chance of answering the question correctly as would be expected. In their paper they already propose using a data set where ($\beta > \eta_{c} t_{s,c}$) to see if this is indeed why this way of combining KCs works well in practice.

The experiment proposed above is put to the test here. Fitting a conjunctive model is hard in practice, but generating data using one is rather straightforward. Whether a real life data set contains many questions where skills are such that $\beta < \eta_{c} t_{s,c}$ cannot be said at this point. Even if this is not the case though, it can be argued that the parameter values can be skewed slightly to ensure that this occurs. The rationale behind is, is that the fitted values obtained from the real data may be skewed towards $\beta < \eta_{c} t_{s,c}$ due to a additive model being used in the fitting process. It would then be expected though that the retrieved parameters using these values would be skewed towards $\beta < \eta_{c} t_{s,c}$ again.

In comparing seIRT's (or eIRT's) parameter values to those of LFA $\beta$ should be multiplied by $\alpha$. $\theta$ is to be multiplied by a weighted average of $\alpha$ (according to the ratio of KCs of the questions that the corresponding student has answered). Finally a weighted average of $\eta$ per KC should be taken according to the ratio of questions each student has answered containing that KC. seIRT can be compared to PFA by combining the steps above with the steps needed to compare PFA to LFA.

\end{comment}

\subsubsection{Combined Model}
The three models introduced above can all be encompassed by a more complex model.

\begin{equation}
P = \sigma(\sum_{c \in KC}\frac{\alpha_c \theta_{s,0}}{|KC|}+\eta_{s} \gamma_{c} g_{s,c} + \eta_{s}\rho_{c} f_{s,c} - \beta_{c})
\end{equation}

LFA can be obtained from this model by taking $\alpha=1$, $\eta=1$ and $\gamma=\rho$. PFA can be obtained from this model by taking $\alpha=1$, $\eta=1$ and $\theta_{0}=0$ (minus the last one for PFA+). The adapted eIRF can be obtained by taking $\gamma=\rho=\alpha$ and realizing that $\beta$ already incorporates $\alpha$. It suffers from the same indeterminacy problem as the seIRT model, which can be dealt with in the same way.

\subsubsection{Model issue}
\todo{Somehow it seems to me that it is a major issue that these models are inconsistent: Students have only a single initial skill, while after learning skills diverge. This is rather odd as forgetting this past and fitting the model again would result in all these different levels being dumped into a single skill again.}


\section{Related work}
\label{sec:RW}
\commd{Right now, this section is a laundry list: it could be improved by adding some more structure.}

In \cite{knowledgeproblem} Beck goes beyond investigating the accuracy of a model (knowledge tracing in this case) and also looks at the parameter values. The authors prime reason for concern lies in identifiability: the fact that widely differing parameter settings can lead to almost identical model outcomes. Although this paper does concern itself with the 'plausability' of parameter values it only goes so far as to nudge the parameter to values deemed plausible rather than asking the more fundamental question of whether the parameter values are useful at all.

Learning factor analysis (LFA) \cite{lfa} is a cognitive model of how students learn. It is a Rasch model\commd{is one of the IRT variants... (you have not mentioned that yet)} extended with a learning rate to model learning over time. In \cite{lfa} LFA is used in the context of a greater cognitive model of learning and skill that includes what knowledge components are linked to what items. The quality of fit (measured as the log likelihood of the entire data set given the model plus some penalty for the number of parameters) is used as a measure of quality of a specific linkage of knowledge components to items.

Performance factor analysis (PFA) is a further extension to LFA. It is introduced in \cite{pfa} as an alternative to knowledge tracing and focuses more on correctly estimating whether a student has mastered a particular knowledge component. The difference with LFA is that learning rate is dependant on whether a question is answered correctly or not. Furthermore initial student knowledge is dropped. The foremost reason for doing so is to make this model work for students of whom no data had been taken into account when fitting the model.

In \cite{ktpfa} Gong et al. also made a comparison between various knowledge tracing approaches and PFA. Whether PFA or KT performs better remains inconclusive. Upon inspecting parameter values they found that many learning rates were negative, which seemed implausible in real life. They noted that upon placing a lower bound of 0 on the learning rate, performance improved.  Additionally the authors used a pretest and correlated the performance on this test to the initial knowledge parameter from the model. In this set-up an adapted version of PFA (the same version that will be used in this research: the original doesn't use initial student knowledge) showed the highest correlation).

In \cite{blackart} Yudelson et al. show some particular factors that can negatively influence the quality of PFA models. One of the factors looked at here is model complexity: on the one hand this is done by using a more finegrained set of KCs and on the other hand by adding another parameter to the PFA model. In evaluating their results the authors did not only look at accuracy measures, but also inspected values for specific parameters. \todo{how did they get their standard deviations for the parameters? They don't seem to have split their data?} In inspecting learning parameters they also noted how the learning parameter for wrongly answered questions are often negative when initial knowledge is not included in the model. They concluded that PFA not so much models student learning in this case, but rather performs some kind of 'error tracking' in order to produce good estimates of skill despite not having any additional information on the student. They therefore prefer an adaptation of PFA that includes initial student knowledge.

In \cite{eirt} Roijers et al. extend the 2PL IRT function to include a learning rate. This new version is called extended IRT (eIRT). There are two important differences in how this model has been extended and how LFA and PFA came to be. First LFA and PFA use a multidimensional model, which means that multiple skills can be associated with a single problem. Moreover in eIRT the learning rate depends on students, while in LFA and PFA the learning rates depend on the skills. Roijers et al. perform experiments using generated data, that shows that parameters can often be retrieved from the data. Additionally they have a real life data-set based on 14 students from three different groups. With the small amount of data in their real life set, they conclude based on their previous experiment that only rule difficulty and student initial knowledge can be determined with enough accuracy. After training the model on the data the fitted initial knowledge of the three different groups confirmed their hypothesis of the ordering of average initial knowledge per group. Also the ordering of difficulty of the rules involved is indistinguishable to rankings made by experts. The authors thus showed that the difficulty and initial knowledge parameters obtained reflect the ordering of these in real life.


\section{Research Question}
\label{sec:RQ}
\todo{Considering to use the term invariant as is done in IRT}
The main question looked into in this thesis is "When are parameter values found in fitting IRT based models distinguishable?". The interpretation of this question is specified in this section and some subquestions are defined which provide structure for the conducted experiments and the discussion of the results. The questions are each formulated in a subsection and referred to throughout the rest of this thesis.

Distinguishable here refers to the need of knowing if the values of parameters are distinct rather than knowing how reliable any particular value is. I.e. the important question would be 'If the value for Tim's initial knowledge is higher than John's is this actually correct?' Thus instead of looking at how reliable particular values obtained in fitting the model are, it is looked at how reliable the ordering of the different values are.

%Distinguishable will not have a clear distinguishable or not answer, but will rather have
There are different levels on which distinguishable can be looked at. In the question above it is on the level of two specific parameters. The default level on which this question will be answered is on the level of parameter type: how well are all the parameters of a particular type distinguishable from each other (macro level). Nevertheless some energy is put into exploring this question on the micro level: i.e. can we say something about how distinguishable a specific parameter is from other parameters?

When on the one hand refers to the fact that distinguishable will depend on some factors, such as how much and what data is used in fitting the model. When also refers to finding conditions that indicate how distinguishable the parameters are. In later sections it is seen that finding the distinguishability of parameters is complex, time consuming and restricted to subsets of the data. Thus finding other, more easily obtained measures that indicate distinguishability would be favorable.

\subsection{What is the influence of the amount of data on distinguishability}
It is expected that simply increasing the amount of data while holding the number of parameters constant will improve the estimation of those parameters and thus increase distinguishability. 

\subsection{How does distinguishability differ between domains?}
There are some quite different ITSs that provide data in a form that is suitable for the IRT based models under investigation. The structure of the data between these ITSs and even between different subjects within the same ITS can be quite different. Additionally, learning is a complex matter for which many differences between classes using the same ITS can already impact how well the model works. Therefor different datasets are looked at to represent different domains.

\subsection{How can we easily know that parameter values are distinguishable?}
This subquestion refers to the second interpretation of 'when' in the main research question: for some measures that are more easily obtained than the distinguishability of parameters it will be checked how well they relate to the distinguishability of parameters, both on the macro level and the micro level.

\subsection{To what extent is distinguishability attributable to stochasticity of the model?}
In section ref{sec:inherent} it was explained how part of the variance is caused by the stochasticity of the model. It is interesting to see what role this stochasticity plays here as the model cannot do better than this bound. Additionally this variance (which is easier to obtain) may correlate with the total variance helping in answering subquestion 3.




\section{Method}


\subsection{Model Performance}

\label{sec:perf}
The most common performance measure used in the context of ITS learning models is some measure of accuracy of model predictions on next-item student performance. Although this research looks into the values of model parameters and is less concerned with other measures of performance an accuracy measure is still used. The main rationale behind this relates to subquestion 4 as the accuracy of a model may hold some relationship to how well the parameters are matched. This would be especially useful as accuracy measures are relatively easy to obtain and are often already looked at in ITSs. 

The specific accuracy measure used for this research is A' (pronounced a-prime) \cite{modelreview}. For this measure two items are represented to the model, one which was answered correctly and one which was answered incorrectly. The model is used to determine which is which. The advantage of this method is that "values of A' are statistically comparable across models and data sets" \cite{modelreview}.

Log likelihood of the data given a model is another measure that plays a large role in this specific context as this is what is maximized in fitting the model. Although the log likelihood value might not be very informative, it does give an indication of how well the model fits the data-set and in combination with the measured accuracy one can get an idea to what extent over-fitting occurs. To be able to compare this value over different runs it will be normalized by dividing by the number of data points.


\todo{currently this part is out of the question. I might bring up this discussion again:
Additionally a method that from IRT \cite{hambleton} will be used. After initially fitting the data will be split in two along skills, such that one set contains all the easiest skills and one will contain all the most difficult skills. Then the fitting will be done again on both new data-sets. If the assumptions of the model were correct the parameters for students found should be roughly the same from both sets. This procedure will be repeated, but then making a split on student initial skill and looking at the skill parameters.}

Although the primary way in which model parameters will be examined is described above, alternative methods will be used where applicable. Some data-sets contain additional information that can be used to more directly draw conclusions on whether found parameter values are meaningful. Two examples from the related research section are \cite{eirt} where expert opinions and an indication of what groups have higher initial knowledge were used and \cite{ktpfa} where a pre-test was used as an indicator of initial knowledge.

\subsection{Rank Order Correlation Measures}
Rank order correlation measures are used when comparing sets of paired values. Rank orders are made for each set meaning that every value is replaced with its rank (i.e. 1 for the highest value, 2 for the second highest). A correlation is then made between the two rank orders, where a value of 1 means that if the one value is the x highest value, the paired value in the other set is also the x highest value. A rank order correlation of -1 means the opposite: if one value is the  highest, the paired value in the other set is the x lowest value. For example we could have weight and height of people as the pairs of values. We would expect a positive rank order correlation measure, since taller people are generally more heavy. Nevertheless we do not expect a value of 1 since a very tall person might be lighter than a slightly shorter person. The advantage of using a rank order correlation is that it indicates that a relation exists between the two values while not restricting this relation to a particular form (e.g. linear relation)

The first rank order measure that will be used here is Kendall's Tau. Kendall's Tau looks at all the pairings of values in one set and looks whether or not the order of those values is the same as the ordering of the paired values. Kendall's Tau is then equal to $\tau=\frac{|same ordered pairs|-|different ordered pairs|}{.5 n (n-1)}$. $n$ is the number of paired values and the  denominator then boils down to the number of possible pairs within a set of values. Here $\tau$ will be used to compare pairs of estimates of parameters. In this case the interpretation is that if we take two random observations from one set and see that observation 1 is bigger than observation 2 in $frac{\tau+1}{2}$ of the cases observation 1 in the other set will also be bigger than observation 2 in that other set.

The other rank order measure used is Spearman's rho. The formula for Spearman's rho is $1-\frac{6 \sum\limits_{i=1}^n d_{i}^{2}}{n(n^{2}-1)}$ Where $d_{i}$ is the difference in rank for the $i$th pair of values and n is the total number of pairs. The difference with Kendall's tau is that more emphasis is put on 'outliers' (large differences in ranks between pairs). Spearman's rho will be used to compare sets of different paired measurements to see if a relation could be found between them, which is more heavily impaired by outliers.

\subsection{Inherent Variance}
Inherent variance is used as the name for the variance caused by the stochasticity of the model. 

In section \ref{sec:inherent} information functions were discussed. The introduction of different learning rates for correct and incorrect answers introduces a problem: the data and the labels are no longer independent and therefor the information function is no longer independent from the labels. This issue makes it difficult or even impossible to create an information function a different approach is taken.

The situation described by Baker in section \ref{sec:inherent} can also be achieved by simulating the model. In the simulated approach, first the parameters are determined by fitting the model on the data (here the labels do have their influence on the result). The found parameters are then used to stochastically generate new labels for the data. This means that if the model predicts a .2 probability that the question is answered correctly, 20\% of the time the label will be 1 and 80\% of the time the label will be 0. In the PFA model this generated answer will also influence further probabilities due to the different learning rates for correct and incorrect answers to questions. Multiple sets of labels are generated for the data in this fashion, after which the same model is fitted again on these sets. The inherent variance of every parameter is then estimated by calculating the variance of that parameter over the different trained models. \todo{ Kendall's Tau rankorder is calculated between the different resulting models.}

\subsection{Observed variance}

In order to get an idea of the variance of parameters in reality, data is split into multiple parts. The same model is fitted on each part of the data (from now on called a split) and the variance for each parameter can then be calculated over the different models. 

The focus here lies on the KC parameters. Also the splitting of the data should retain the way the data might be seen by an ITS in real life. Therefor the data is split on the basis of students: every split contains the data of some of the students and every student in a split does not occur in another split. The same split is done on the test set such that the model trained on the data of specific students is also tested on the test set of those particular students.

To investigate subquestion 2, different splits are made, with a decreasing number of students per split. It should be noted that this means that for the larger splits, a lower number of parameter values are present and that thus the second order variance will be higher.

\begin{comment}
\subsection{Representation of variance}
The variances discussed in the previous sections are not easily interpretable as it is not directly clear whether a variance is large or small for a particular parameter. To account for this problem the following reliability measure for every parameter value will be used:

\begin{equation}
\label{eq:reliability}
R(\beta_{n}) = 1-\frac{\sigma^{2}(\beta_n)}{\sigma^{2}(\beta_{n})+\sigma^{2}(\beta)}
\end{equation}

Where $\beta_{n}$ is the $\beta$ parameter of the n-th KC, $\sigma^{2}(\beta_{n})$ is its variance and $\sigma^{2}(\beta)$ is the variance over the $\beta$ parameter of all KCs in the model. The reliability can be calculated for both the inherent and observed variance and can be calculated for any parameter, although $\beta$ was taken as an example here. Two issues should be noted here. First a normal distribution of the parameters is assumed here and when calculating from different reliabilities of different parameters the harmonic mean should be used.

\begin{figure}
\includegraphics[width=\textwidth]{images/reliVSprob2.png}
\caption{Relation between reliability and percentage of correctly distinguished parameters}
\label{fig:reli}
\end{figure}

When considering the consistency of parameters, a good way to look at them is distinguishability. For example, if we were to look at every possible pair of parameters, in how many cases would the one that is seemingly largest, actually be the largest? The reliability over the average variance of a parameter type is directly related to this measure (see figure \ref{fig:reli})
\end{comment}

\subsection{Data cleaning}
\label{sec:cleaning}
Splitting the data may exacerbate some of the issues that can be encountered in fitting the model. One example is when all questions associated with a student or a KC are answered correctly or incorrectly. This makes the fitting algorithm want to assign infinite values to parameters. Another problem is when for a KC there is such a limited number of questions answered that learning rates cannot be estimated.

To prevent these issues the following steps are taken. On the whole dataset, students that answer all questions correctly or incorrectly are removed. In every split KC's for which every question is answered correctly or incorrectly is not taken into account for that split. Additionally if for a KC there is less than two questions answered correctly or incorrectly that KC is not taken into account for that split. This means that this KC is removed from the items and any item that no longer contains a KC is dropped from the data. It is assumed that this does not cause students to now have answered all questions correctly or incorrectly, but this assumption will be checked during the experiments to be certain.


\subsection{Domains}
\label{sec:domain}
In generating the data there are many other factors that might play a role in the performance of the models. Among those factors are the values that the parameters will be given, the distribution of knowledge components over the items, the ratio of students to items etc. As indicated in section \ref{sec:RQ} these factors are not explored methodically and extensively, but rather a few different datasets are used to represent some of the variation naturally found in this kind of data. The different datasets and some of the characteristics are described in this section.

\subsubsection{Bridge to Algebra}
The first data-set is one of the datasets used in the 2010 KDD cup on education data mining. The data is from Carnegie Learnings' Cognitive Tutor Software meant for high school-students aged 15-18. \cite{ct} provides an overview of some of the features of this program. The data was obtained from the Bridge to Algebra course during the school year 2006-2007 \cite{bridge}. 

\subsubsection{Algebra I}
This dataset was also provided in the 2010 KDD cup. The data is also obtained from Carnegie Learnings' Cognitive Tutor Software, but from the Algebra I course during the school year 2005-2006 \cite{algebra}

\subsubsection{Assistment}
This dataset is quite different from the other two as it is taken from a web-based ITS named Assistment whose details and development story can be found in \cite{razzaq}. The data is from 12 to 14 year old students and was used in \cite{ktpfa} which was discussed in section \ref{sec:RW}. The data does not only contain data from usage of the ITS but also data from a pre-test.

\subsubsection{Data information}
\begin{table}
    \begin{tabular}{| l | l | l | l |l|}
    \hline
    Dataset & \# questions & \# KCs(cleaned) &\# students(cleaned) & \% correct (testset) \\ \hline
    Bridge & 1,814,398 & 455(34) & 1129(17) & 83 (80)\\ \hline
    Algebra & 585,557 & 104(6) & 564(10) &  76 (75)\\ \hline
    Assistment & 110,842  & 106(0) & 425(20) & 65 (59)\\
    \hline
    \end{tabular}
\end{table} 
   
\begin{table}
	\begin{tabular}{| l | l | l | l |l | l |}
    \hline
    Dataset & 1 & 2 & 3 & 4 & 5-7\\ \hline
    Bridge & 1801254 (0.993) & 11406 (0.006) & 1623(0.001)&113(0.000)&2(0.000) \\ \hline
    Algebra & 368875(0.630)  & 114600 (0.196) & 85818 (0.147)&5581 (0.010)&10683 (0.018) \\ \hline
    Assistment & 56521 (0.510) & 37356 (0.337) & 11770 (0.106) & 4237 (0.038)&958 (0.009)\\
    \hline
    \end{tabular}
\end{table}  

\begin{comment}
2(0.000)& 0&0
6455 (0.011)& 4123(0.007)&105(0.000)
958 (0.009)&0&0
\end{comment}

\subsection{KC Experiment}
\subsubsection{The Experiment}
In detail the experiment for KC parameters consists of the following steps per model:
\begin{enumerate}
\item Clean the data by removing students according to section \ref{sec:cleaning}
\item Split the data in a increasing number of parts. Then for each part:
\begin{enumerate}
\item Clean the data by removing KCs according to section \ref{sec:cleaning}
\item Train the model and determine the variance for every parameter type
\item Determine performance on the test-set for that split
\item Determine the inherent variance through the Fisher matrix and through simulation
\item Normalize the found variances with the variances per parameter type found above
\end{enumerate}
\item Determine the variance per parameter over all the splits
\item Normalize the found variances with the average of variances per parameter type found above
\end{enumerate}

\subsubsection{Representing the Results}
\begin{comment}
Dit is een voorstel voor het representeren van de resultaten. Feedback, ideeen of suggesties worden erg gewaardeerd.
For every split into a number of parts and every model the following will be represented:
\begin{enumerate}
\item The average reliability for each parametertype (both inherent and observed \todo{ preferably the reliability of the difference between the two as well, even though that has not worked too well in the testrun I have shown, although I'm less than 90\% sure I did not make a mistake})
\item The average accuracy and normalized log likelihood
\item Histograms that display the reliabilities of the different parameters. Outliers can now be easily spotted.
\end{enumerate}
\todo{I'd also like to see to what extent inherent and total variance are related over the different splittings and models, as well as relations with accuracy and normalized log likelihood. correlation or spearmans R are two options I'm considering}
\end{comment}

\section{results}
\subsection{Orderings within parameters}
In figure \ref{fig:ranks} the results on first glance are generally as is expected: the tau of parameters improves as more data is used and the rank orders of simulated data are higher than those over splits of observed data. At a second look a few things stand out. Sometimes there seems to be some noise in the experiments (for example the $\eta$ parameter in the Algebra dataset). Furthermore the increase in tau for $\rho$ is small, if existent at all, for the tau over simulated data. Another interesting observation is that the rank orders on the observed data get closer to the rank orders of the simulated data as more data is used and that the gap between the two is smaller for $\beta$ than it is for the learning parameters. Finally it would seem that there is still room for improvement for Tau when adding more data.

\begin{figure}

\centering
\subfloat[Legend for the figures]{
  \includegraphics[width=30mm]{images/legend.png}
}
\subfloat[Rankorders for Bridge0506 parameters]{
  \includegraphics[width=65mm]{images/bridgeallmodranksKC.png}
}
\hspace{0mm}
\subfloat[Rankorders for Algebra0607 parameters]{
  \includegraphics[width=65mm]{images/algebraallmodranksKC.png}
}
\subfloat[Rankorders for Assistment parameters]{
  \includegraphics[width=65mm]{images/gongallmodranksKC.png}
}
\caption{Kendall's Tau rank orders of the different parameter-types}
\label{fig:ranks}
\end{figure}

In comparing the AFM and PFA model the rank orders of the $\beta$ parameters, and those of the $\eta$ and $\gamma$ parameters of both models are near each other. It is against expectations that the $\gamma$ parameters rank orders are so high as a simple expectation would be that both $\gamma$ and $\rho$ parameters would show lower rank orders as there is fewer data available to fit both of them.
 
The Tau for the different domains look quite similar, which is surprising given the differences in the amount of data per KC for the different domains. The improvements in Tau for the Bridge data set seem to taper off towards the end and will probably benefit least from more data, while the other models still seem to have improving tau values. 
\subsection{Overall variance of parameters}
\label{sec:varresults}
\begin{figure}[!htbp]

\centering
\subfloat[Legend for the figures]{
  \includegraphics[width=30mm]{images/legend.png}
}
\subfloat[Standard deviations for Bridge0506 parameters]{
  \includegraphics[width=65mm]{images/bridgeallmodsdsKC.png}
}
\hspace{0mm}
\subfloat[Standard deviations for Algebra0607 parameters]{
  \includegraphics[width=65mm]{images/algebraallmodsdsKC.png}
  \label{fig:sdalg}
}
\subfloat[Standard deviations for Assistment parameters]{
  \includegraphics[width=65mm]{images/gongallmodsdsKC.png}
}
\caption{(Average) standard deviations of the different parameters}
\label{fig:sds}

\end{figure}

Figure \ref{fig:sds} is similar to figure \ref{fig:ranks} except with standard deviations over the different splits or generated data sets instead of Kendall's Tau. The diminishing standard deviations are as expected, but both the large values as well as the small difference between variance over splits and variance over generated data sets is surprising given the graphs in figure \ref{fig:ranks}. 

The first might be explained if the different splits give very different parameter values while still largely remaining the same ordering. The second problem would still not be explained though. Upon inspection it turns out that somehow the variation within a parameter of a model fitted on a split is far larger than that of a model fitted on the whole dataset (which was used to normalize the standard deviation values). Additionally the variance within a parameter of a model fitted on generated data is again larger than that of a model fitted on a split. This makes that the same variance has less influence on the ordering since the nearest values of other parameters are further away. Here also lies the explanation for the out-lier of $\rho$ in figure \ref{fig:sdalg}: upon inspection, a model fitted on one of the six splits and its simulated data showed an unusually high standard deviation for $\rho$, having a big impact on the final standard deviations.

\todo{Ik heb deze data in een onhandig formaat en zou het kunnen agregeren en weergeven om een beter beeld te hebben van hoe dit er precies uitziet en varieert per dataset/parameter/aantal splits}

\subsection{Measures of fit}
In figure \ref{fig:likely} the points belonging to the different splits can easily be distinguished: there are three groups one for each data set (from left to right, Assistment, Algebra and Bridge) and within each group the point with the highest tau value is the one with the most data associated with it. That the PFA models have a higher likelihood is to be expected as it has more parameters than the AFMs. It is also sensible that with more data in a split the likelihood goes down, since there the same number of parameter values are used to explain more data. Although there is a nice relationship between tau and likelihood within each dataset, this relationship might mostly stem from the previous two relationships mentioned: as the amount of data goes up, so do the tau values and as the amount of data goes up, the likelihood goes down. When looking over all three datasets there actually seems to be a slightly positive relation between likelihood and tau values. The wide spread between splits of the same dataset (where the relation is quite negative) compared to the slight positive relation makes it unlikely that likelihood can be used to make inferences about the invariability of parameter values.   

\begin{figure}[!htbp]
\centering
\subfloat[Average Log Likelihood vs Kendall Tau over splits]{
  \includegraphics[width=65mm]{images/alllikely.png}
  \label{fig:likely}
}
\subfloat[a' vs total Kendall Tau over splits]{
  \includegraphics[width=65mm]{images/allaprimes.png}
  \label{fig:ap}
}
\caption{Kendall Tau values vs goodness of fit measures}
\end{figure}

A' shows more promise to be a useful feature in establishing the invariability of parameter values. As seen in figure \ref{fig:ap} there is a positive relation between A' and the tau of all the different parameters in both models. Another interesting observation is that the PFA models generally performs better on A' than the AFMs do.

\subsection{Micro level}
So far the results have been looked at on a global level, now it is time to look at the data from the perspective of individual parameter values. In table \ref{tab:intorank} the Spearman's r values can be seen when comparing the inherent variances and the observed variances of all the different parameter values from all the different splits. The values found are quite high, indicating that inherent variance could well be used in predicting the variance of specific parameter values. 

In section \ref{sec:varresults} it was discovered that over the different splits there can be a significant difference in the variance of a parameter. This has as an unintended consequence that the Spearman's r values are higher when the values of the different splits overlap less. Therefor the Spearman's R values were recalculated using the average of the values in each split rather than the value over all splits combined. The results were vary similar (mostly a difference of .01) and therefor left out. 
\begin{center}
\begin{table}[!htbp]
\begin{tabular}{| l || l | l ||l|l |l|}

    \hline
     & AFM  & & PFA & &   \\ \hline
    Dataset & $\beta$ & $\eta$ & $\beta$ & $\gamma$ & $\rho$  \\ \hline
    Bridge     & .86 & .92 & .91 & .95 & .92 \\ \hline
    Algebra    & .91 & .97 & .94 & .98 & .96 \\ \hline
    Assistment & .90 & .97 & .88 & .97 & .96 \\ \hline \hline
    Overall    & .87 & .94 & .91 & .96 & .94 \\
    \hline
\end{tabular}
\label{tab:intorank}
\caption{Spearman's R of inherent and total variance of various parameters and models over all splits}
\end{table}
\end{center}

\todo{
I have made graphs scatterplotting value in the mainmodel against both inherent variance found in the splits and the observed variance over the splits, but am not sure to include them as I'm not quite sure what I would have liked to learn from them. Interesting observation though is that variance doesn't appear to be higher on the edges.
Interesting still might be to see how such a figure compares to seeing the inherent variance based on the model on whole the data instead of the splits.}

Prediction of knowledge level...



\newpage
\todo{
\section{Things to add and change}
\begin{itemize}
\item Describe terms and use those consistently!
\item related work is a bit out of place as it is not very intelligible without information given later. Maybe move it to the end, or rather integrate it with other stuff?
\item Investigate some of the outliers? 
\end{itemize}
}
\bibliographystyle{alpha}   % this means that the order of references
			    % is determined by the order in which the
			    % \cite and \nocite commands appear
\bibliography{litlist}
\newpage
\appendix
\section{Implementation and Mathematical argumentation}
\label{app:math}
The AFM and PFA model are relatively straightforward in their data representation and implementation. For these models $x$ in formula \ref{eq:logistic} is linear in the parameters, which means that standard logistic regression can be applied. In this appendix first the way the data is represented is described followed by a proof that logistic regression indeed finds the parameter values where the likelihood of the data is highest. 
\subsection{Data Representation}
For logistic regression the data is represented in a matrix $\Phi$ such that $\Phi w$ is equal to a vector of each value of $x$ in formula \ref{eq:logistic}, where $w$ is a column vector of the parameter values. 
In this matrix the rows represent data points while the columns represent what the parameters should be multiplied with. The dimensions of the matrix are thus equal to the number of data points by the number of parameters.

In the formulas for AFM (formula \ref{eq:afm}) and PFA (formula \ref{eq:pfa}) $x$ consists of a sum where every part contains exactly one parameter. This makes construction of $\Phi$ straightforward: in each row (thus for every data point) a 1 is placed for every present parameter that stands isolated (this goes for $\theta$ and $\beta$) and for the others ($\eta$,$\gamma$ and $\rho$) the right value for that data point is inserted (a non-negative integer). Any parameter not used for that specific datapoint will have a value of 0.

\subsection{Workings of Logistic Regression}
Logistic regression estimates the values of the parameters that maximize the likelihood of the data given the model. The likelihood of the data is equal to $\prod_{d \in D} P_{d}^{t_d}  (1- P_{d})^{1-t_d} $

\pagebreak 
\section{Glossary}
\todo{
Making a start with using terms consistently, plus I generally feel that a glossary would have helped me greatly in understanding papers etc.}
\begin{description}
\item[Answer]Whether a question was answered correctly or incorrectly
  \item[Item] A problem (step) in the ITS to which a single answer can be given
  \item[Knowledge Component] A skill, piece of knowledge etc. that is associated with one or more items and in which students can have a level of competence (named 'skill' here)
  \item[paramater] A type of parameter used in one of the models (e.g. $\beta$)
  \item[paramater value] The value of a parameter belonging to a specific case (e.g. the value of the $\theta_{0}$ parameter belonging to student nr 10)
  \item[Question] An instance of an item, as asked to a single student
  \item[Skill] Level of skill of a single student
\end{description}

\end{document}
