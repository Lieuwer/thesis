\documentclass{scrartcl}
% \documentstyle{article}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{caption}
\begin{comment}

\end{comment}
\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{#1}}

\DeclareCaptionType{mycapequ}[][List of equations]
\captionsetup[mycapequ]{labelformat=empty}

\providecommand{\comm}[1]{{\bf[ #1 ]}}
\providecommand{\commd}[1]{\comm{D: {#1}}}

\begin{document} 
\title{Master Thesis Research Proposal}
\subtitle{What can the parameters of IRT based learner models tell us?}
\author{Lieuwe Rekker}
\maketitle
\nocite{labelcombi}
\nocite{lftransfer}
\nocite{importance}
\nocite{knowledgeproblem}
\nocite{modelreview}
\nocite{eirt}
\nocite{pfa}
\nocite{ktpfa}
\nocite{skillcombi}
\nocite{lfa}
\nocite{blackart}
\nocite{hambleton}

%\listofmycapequs

\section{Introduction}
\begin{comment}
Many current intelligent tutor systems (ITS) use learner models that can give indications of to what extent a student has mastered a particular skill and even how fast students learn.  The quality of these models is generally measured by how well these models predict 'one question into the future' performance, which is what these algorithms\commd{You didn't mention any algorithms yet, just models - and even then, they maximimize the likelihood of the training data...} are made to maximize. Parameter values from these models are also inspected and stated to represent students' knowledge levels, how fast students learn, how difficult questions are etc. It would thus be wise to inspect other factors beyond the accuracy of the model to gain some idea of under what conditions they actually convey some stable real world factor or are rather part of a more black-box like model that performs well on its prediction task, but whose parameters are otherwise meaningless. \cite{knowledgeproblem} shows that this is indeed a problem: widely differing parameter settings can yield similar, (near) optimal performances. This proposal proposes research to explore if parameters of learner models hold meaning in real life.

The approach of this research is from the perspective that models resemble reality, but are necessarily not precise mirrors of it. Initially generated data is used to see how well parameter values can be retrieved at all and to see what happens to parameters when there is a mismatch between model and 'reality' (i.e. training a model on data generated by a different model). The amount of training data is looked at especially, since even if stable parameter values can be found, this will probably break down at some point. In the case of real data, original parameters and models are unknown. Over multiple training runs on different data-sets, variation in parameter values can be inspected. High variation here indicates that (despite possible good prediction performance of the model as a whole) the parameter does not represent anything in reality.
\end{comment}
\commd{Provide short motivation for ITS's first.} Today more and more intelligent tutor systems (ITS) are being used, both in research settings and in the world at large. An ITS is a computer program used by students to learn about different subjects and generally entails that students solve problems within the system. Data from these systems have been used to build models of how students learn.\commd{Most learner models do that indeed, but some restrict themselves to the probability of a correct answer... We don't like those, but still.} These learner models in turn have been used in ITSs to estimates the level of the student and adjusts the problems it presents to the student accordingly.\commd{Reformulate in terms of possible usuage of student models.} In research these learner models are generally evaluated by looking at some measure of how the performance predicted by the model fits the real data.\commd{Will need some cites. And, I would cute the word real.} In this research a set of learner models based on item response theory (IRT) will be discussed and it will be examined when the parameters that are fit in these models are stable.\commd{Why? motivate please}

In psychology and more notably psychometrics, how students perform on problems has long been a field of research. The activity in this field was hastened by the necessity for standardized testing. This has eventually led to the development of item response theory (IRT) (for a good overview see \cite{hambleton}). In the models stemming from IRT, students are characterized by a skill level and problems are characterized by factors such as difficulty and discrimination. Through decades of research and practical use (especially for standardized tests) IRT has gained a solid theoretical and experimental basis, which makes it a logical basis for use in ITSs in estimating the level of students. Problematic though in the application of this theory to ITS data is that learning is not taken into account.\commd{Alternative, ...is that IRT assumes the compentence of the students to be constant for the duration of the test. In ITS, which are designed to help students learn, this can obviously not be assumed.} Within a test learning is not much of an issue, but in an ITS data is collected over longer time spans and learning is actually meant to occur. \commd{ from here onwards the structure is a bit illogical. First cluster all information you want to give about IRT by itself first, then mention why you want to extend, then mention the how, then the problems that one can encounter by doing so, that a comprehensive evaluation and comparison of these models has not yet been made - and then state what your research will be, (and maybe mention the use of your research for educators and researchers)} Therefore the adaptations of IRT models used on this kind of data incorporates a learning rate.

In IRT there are established methods to see if obtained parameters are significant as well as methods that check whether its assumptions are met. This gives confidence that found parameters for students and problems are meaningful and can be used to make statements about student skill and item difficulty. In the case of the models used for ITSs, generally only model fit compared to the data (in various forms, most notably one-step-ahead prediction accuracy) is taken into account. There are some exceptions where concerns are raised on the plausibility of parameters \cite{ktpfa}, \cite{knowledgeproblem} and accuracy of parameters \cite{blackart}. Not looking further into the accuracy of these parameter estimates is at the least a missed opportunity (e.g. knowing how easy certain skills are to learn, how difficult certain items are, or how fast a particular student learns can be valuable information, even if simply to improve the ITS) and might already be used wrongly in some cases (for example making statements on the difficulty of questions, without knowing whether the fitted values can be trusted).

This research will look at the influence of how certain factors influence the accuracy of parameters estimation. When applying IRT to test results, generally no external measure is used to check the found difficulty and skill parameters against, rather internal validity is used to see if the assumptions of IRT hold \cite{hambleton}. For example, when estimating the difficulty of an item, it should not matter whether the results of low or high ability students are used to estimate the difficulty. If this is not the case, this indicates that some assumptions are not met and makes the parameter values found of questionable value. This research will take a similar approach in looking at the found parameters. The research will be split into two parts. In the first part generated data is used, so that the real parameter values are known and effects of different factors can be estimated. In the second part real data will be cut into pieces so that variance of parameters can be determined. In combination with the first part of the research it is hoped that conclusions can be drawn as to whether the parameter values found are useful and whether the data-fit measures normally looked at provide some clue to this usefulness.

\section{Related work}
\commd{Right now, this section is a laundry list: it could be improved by adding some more structure.}

In \cite{knowledgeproblem} Beck goes beyond investigating the accuracy of a model (knowledge tracing in this case) and also looks at the parameter values. The authors prime reason for concern lies in identifiability: the fact that widely differing parameter settings can lead to almost identical model outcomes. Although this paper does concern itself with the 'plausability' of parameter values it only goes so far as to nudge the parameter to values deemed plausible rather than asking the more fundamental question of whether the parameter values are useful at all.

Learning factor analysis (LFA) \cite{lfa} is a cognitive model of how students learn. It is a Rasch model\commd{is one of the IRT variants... (you have not mentioned that yet)} extended with a learning rate to model learning over time. In \cite{lfa} LFA is used in the context of a greater cognitive model of learning and skill that includes what knowledge components are linked to what items. The quality of fit (measured as the log likelihood of the entire data set given the model plus some penalty for the number of parameters) is used as a measure of quality of a specific linkage of knowledge components to items.

Performance factor analysis (PFA) is a further extension to LFA. It is introduced in \cite{pfa} as an alternative to knowledge tracing and focuses more on correctly estimating whether a student has mastered a particular knowledge component. The difference with LFA is that learning rate is dependant on whether a question is answered correctly or not. Furthermore initial student knowledge is dropped. The foremost reason for doing so is to make this model work for students of whom no data had been taken into account when fitting the model.

In \cite{ktpfa} Gong et al. also made a comparison between various knowledge tracing approaches and PFA. Whether PFA or KT performs better remains inconclusive. Upon inspecting parameter values they found that many learning rates were negative, which seemed implausible in real life. They noted that upon placing a lower bound of 0 on the learning rate, performance improved.  Additionally the authors used a pretest and correlated the performance on this test to the initial knowledge parameter from the model. In this set-up an adapted version of PFA (the same version that will be used in this research: the original doesn't use initial student knowledge) showed the highest correlation).

In \cite{blackart} Yudelson et al. show some particular factors that can negatively influence the quality of PFA models. One of the factors looked at here is model complexity: on the one hand this is done by using a more finegrained set of KCs and on the other hand by adding another parameter to the PFA model. In evaluating their results the authors did not only look at accuracy measures, but also inspected values for specific parameters. \todo{how did they get their standard deviations for the parameters? They don't seem to have split their data?} In inspecting learning parameters they also noted how the learning parameter for wrongly answered questions are often negative when initial knowledge is not included in the model. They concluded that PFA not so much models student learning in this case, but rather performs some kind of 'error tracking' in order to produce good estimates of skill despite not having any additional information on the student. They therefore prefer an adaptation of PFA that includes initial student knowledge.

In \cite{eirt} Roijers et al. extend the 2PL IRT function to include a learning rate. This new version is called extended IRT (eIRT). There are two important differences in how this model has been extended and how LFA and PFA came to be. First LFA and PFA use a multidimensional model, which means that multiple skills can be associated with a single problem. Moreover in eIRT the learning rate depends on students, while in LFA and PFA the learning rates depend on the skills. Roijers et al. perform experiments using generated data, that shows that parameters can often be retrieved from the data. Additionally they have a real life data-set based on 14 students from three different groups. With the small amount of data in their real life set, they conclude based on their previous experiment that only rule difficulty and student initial knowledge can be determined with enough accuracy. After training the model on the data the fitted initial knowledge of the three different groups confirmed their hypothesis of the ordering of average initial knowledge per group. Also the ordering of difficulty of the rules involved is indistinguishable to rankings made by experts. The authors thus showed that the difficulty and initial knowledge parameters obtained reflect the ordering of these in real life.


\section{Research Question}
\label{sec:RQ}
The main question looked into in this thesis is "When are parameter values found in fitting IRT based models consistent". The terms when and consistent require some elaboration and explanation. This is given in the following sub-sections, along with the formulation of related sub-questions.

\subsection{What does 'consistent' mean}
In this context consistent means that parameter values obtained through fitting the model on different data from the same domain leads to finding the same values. In other words the values obtained for a parameter show low variance. Since the data is binary, whereas the model works with probabilities, part of this variance stems from the workings of the model. To recognize how great of an influence this has on the variance the first subquestion is: "What part of inconsistencies in the parameter values can be attributed to the stochasticity of the model?"

Beyond variance caused by the stochasticity of the model, no other specific sources of variance is looked into. Only the total variance is inspected and evaluated to see if it diminishes the value of the measurements.

\subsection{When: what situations are investigated}
The 'when' part refers to the conditions under which the parameter values are consistent. Three factors are investigated here to see the impact on variance of the parameters. First the amount of data used in training is generally a factor that can cause a significant amount of noise. The subquestion that arises is thus "What is the influence of the mount of data on the consistency of the parameters?" 

There are some quite different ITSs that provide data in a form that is suitable for the IRT based models under investigation. The structure of the data between these ITSs and even between different subjects within the same ITS can be quite different. Additionally, learning is a complex matter for which many differences between classes using the same ITS can already impact how well the model works. Therefor different datasets is looked at to represent different domains. This enables looking into the subquestion: "What is the influence of the domain on the consistency of the parameters?".

The idea behind this research is to create a framework from which to look into the obtained parameter values. This analysis would generally not be practical to repeat on every new dataset and thus another subquestion arises: "How can we easily know that the parameter values are consistent?" For example it might be that the performance of the model on a test set is a good indication for the consistency of the parameters.

\begin{enumerate}
\item What part of inconsistencies in the parameter values can be attributed to the stochasticity of the model?
\item What is the influence of the amount of data?
\item What is the influence of the domain?
\item How can we easily know that the parameter values are consistent?
\item (Does removing inaccurate parameters from the data improve the accuracy of other items?)
\end{enumerate}


\section{Models}
All models used in this research are based on item response theory and use a version the 2 parameter item response function (IRF) \commd{No they don't, the first couple you mention use the 1 parameter IRT, also called Rasch, model}. The basis of this function is the logistic ogive function \ref{eq:logistic}. The most basic 2 parameter IRF is shown in \ref{eq:irt}. Here i stands for item and s stands for student. $\alpha$ is the discriminatory power of the item i.e. how much knowledge matters in how well a student does on that item. $\beta$ is the difficulty of the item i.e. how high a students skill should be to achieve a P of .5. $\theta$ is the skill of a student, indicating how well the student has mastered a knowledge component. The value of the function is interpreted as the probability that the student answers a question correctly. This is the same interpretation that is used throughout all models in this research.

\begin{equation}
\label{eq:logistic}
\sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}

\begin{equation}
\label{eq:irt}
P = \sigma(\alpha_{i} (\theta_{s} - \beta_{i}))
\end{equation}

All models used in this research employ an important extension: it incorporates learning by students. Skill, $\theta$ is split up into an initial part and a learning rate, so that each time a question is answered the skill of the student increases. In the ITS there are many different knowledge components and thus the parameters are fit per knowledge component. Depending on the data-set used an item can have multiple components associated with them, creating the necessity to combine multiple item response functions.

\todo{Say something about the fitting procedure for each seperate model.}\commd{what, here? or do you mean the alternate ML optimization of student and item parameters? }

\commd{List the assumptions each model makes. Either in each subsection, or in a separate subsection}

\subsection{Learning Factor Analysis (LFA)}
\label{sec:LFA}


LFA uses a simplified version of the IRF, but extents it by introducing a learning rate as discussed in the introduction and by allowing multiple knowledge components to be associated with a single item. The combination of KCs is made by summing the learned part of knowledge and the difficulty of the KC for every KC that is linked to the item.

\begin{equation}
P = \sigma(\theta_{s,0} + \sum_{c \in KC}  \eta_{c} t_{s,c} - \beta_{c})
\end{equation}
\commd{So the start competence is the same for every knowledge component/skill, but the learning speed can vary per KC/skill... that is an assumption that is explicitly rejected in the eIRT paper.}

The simplification of the model is done by dropping $\alpha$ (this model is called the Rasch model). The splitting of $\theta$ leads to the introduction of an initial skill $\theta_{0}$ defined per student, a learning rate $\eta$ defined per KC (i.e. the KC determines how fast or slow learning occurs) and a number of times that a student has seen items belonging to this particular knowledge component $t_{s,c}$. Please note that in the the original LFA $\beta$ is added. It is subtracted here to maintain similarity to the original IRF and ensure uniformity with the other models used. This has no other effect than that the signs for $\beta$ are reversed.

When looking at a data set where only a single knowledge component is linked to every item, $\beta$ and $\theta_{0}$ are not independent: we can raise both by any amount. To solve this issue, the average value of $\theta_{0}$ is set to 0.
\subsubsection{Combining knowledge components}
\label{sec:comb}
Something more can be said about the way KCs are combined. In \cite{skillcombi} Cen et al. show that in practice there is no difference in performance between an additive model (as used here) or a conjunctive model (where probabilities of individual KCs are multiplied). Nevertheless the authors already mention that this is probably the case because for most KCs $\beta < \eta_{c} t_{s,c}$, meaning that adding KCs does decrease the chance of answering the question correctly as would be expected. In their paper they already propose using a data set where ($\beta > \eta_{c} t_{s,c}$) to see if this is indeed why this way of combining KCs works well in practice.

The experiment proposed above is put to the test here. Fitting a conjunctive model is hard in practice, but generating data using one is rather straightforward. Whether a real life data set contains many questions where skills are such that $\beta < \eta_{c} t_{s,c}$ cannot be said at this point. Even if this is not the case though, it can be argued that the parameter values can be skewed slightly to ensure that this occurs. The rationale behind is, is that the fitted values obtained from the real data may be skewed towards $\beta < \eta_{c} t_{s,c}$ due to a additive model being used in the fitting process. It would then be expected though that the retrieved parameters using these values would be skewed towards $\beta < \eta_{c} t_{s,c}$ again.

\subsection{Performance Factor Analysis (PFA)}
PFA is a direct extension of LFA. In PFA separate learning rates are used for questions answered correctly and questions answered incorrectly. Additionally $\theta_{0}$ is dropped. As put forward in \cite{pfa} $\theta_{0}$ is dropped because this extension is made mostly to make this model more useful in ITSs. Leaving out any student specific parameter makes the model more easily applicable to students not used in the fitting procedure. As noted in both \cite{ktpfa} and \cite{blackart}, leaving out $\theta_{s,0}$ makes parameter estimates worse. Since prediction for students who were not part of the fitting procedure is not a primary concern here, a model that does include $\theta_{s,0}$ (as done in \cite{ktpfa} and \cite{blackart}) is used instead of PFA and will be referred to as PFA+.

\begin{equation}
P = \sigma(\theta_{s,0} \sum_{c \in KC}  \gamma_{c} g_{s,c} + \rho_{c} f_{s,c} - \beta_{c})
\end{equation}
\commd{Is there a minus sign missing here? same assumption as the last, but two different learning rates. }

Here $\gamma$ is the learning rate of the KC for correct answers and g is the number of questions answered correctly. Consequently $\rho$ is the learning rate of the KC for incorrect answers and f is the number of questions answered incorrectly. Just as with LFA above the sign for $\beta$ was reversed compared to the original.

\begin{comment}
To compare the PFA parameter values to LFA values, the weighted average (according to the ratio g:f for each problem) of $\gamma$ and $\rho$ should be compared to $\eta$. $\theta$ cannot be compared when comparing PFA on LFA data, vice versa, $\theta$ should be compared to 0. In case of PFA+ $\theta$ values can be compared directly.

\begin{mycapequ}
\begin{equation}
\eta_{c}: \frac{\gamma_{c} g_{c} + \rho_{c} f_{c}}{g_{c}+f_{c}}
\end{equation}
\caption{Comparison of LFA parameters to PFA parameters}
\end{mycapequ}
\end{comment}

\subsection{extended Item Response Theory (eIRT)}
\label{sec:eirt}
The extended Item Response Theory model by Roijers et al \cite{eirt} is the most straightforward extension to the standard IRT model.

\begin{equation}
\label{eq:eirt}
P = \sigma(\alpha_{c} (\theta_{s,0} + \eta_{s} t_{s,c} - \beta_{c}))
\end{equation}
\commd{Only one competence and learning speed.}

Similar to LFA $\theta$ is replaced by initial skill and a learning rate. Here the learning rate is taken per student though rather than per knowledge component as is done in LFA and PFA. With $\theta$ split up, it would seem that $\alpha$ obtains a slightly different meaning. For $\theta_{0}$ it still has the same discriminatory function. When looking at $\eta$ though, $\alpha$ directly impacts it as a modifier, making learning easier (>1) or more difficult (<1).

\todo{Actually I have no clue on the correct mathematical vernacular, so please nudge me to the right terms to make this more understandable/consice}
It should be noted that different parameter settings can lead to exactly the same model. E.g. all $\theta$s and $\beta$s could be increased by the same amount and the model would still be the same. To still be able to compare parameter values and variances the parameters should be normalized as follows. The average of $\theta{s,0}$ will be set to zero as to resolve the dependency with $\beta$ values. The standard deviation of $\theta$ will be set to 1 as to fix the dependency between $\alpha$ and the other parameters.

\subsubsection{Adapting eIRT}
eIRT as defined by Roijers et al does not incorporate multiple skill steps. In order to be trained on multi-skill data and to be similar to the other models \ref{eq:sumeirt} will be used as a multi-skill extension of the eIRF. To distinguish this version from the original eIRT, this extended version will be denoted as seIRT. A notable difference here is that $\theta_{s,0}$ is divided by the number of KCs involved. This is because $\theta_{s,0}$ should only be added once just as in LFA/PFA, but nevertheless it should be modified by the corresponding $\alpha_{c}$s as well.

\begin{equation}
\label{eq:sumeirt}
P = \sigma(\sum_{c \in KC} \alpha_{c}(\frac{\theta_{s,0}}{|KC|} + \eta_{s} t_{s,c} - \beta_{c}))
\end{equation}

Please note that in the multidimensional case the dependency between $\theta{s,0}$ and $\beta$ is most likely no longer there. The dependency between $\alpha$ and the other parameters still exists though and should still be resolved by fixing the standard deviation of $\theta_{s,0}$ to 1.

\begin{comment}
In comparing seIRT's (or eIRT's) parameter values to those of LFA $\beta$ should be multiplied by $\alpha$. $\theta$ is to be multiplied by a weighted average of $\alpha$ (according to the ratio of KCs of the questions that the corresponding student has answered). Finally a weighted average of $\eta$ per KC should be taken according to the ratio of questions each student has answered containing that KC. seIRT can be compared to PFA by combining the steps above with the steps needed to compare PFA to LFA.

\commd{Let's discuss this: $\eta_s$ is constant per student, and alpha should be inside the sum if you want ot avg it. }


\begin{mycapequ}[!ht]
\begin{equation}
\begin{aligned}
    \beta_{c}:  &  \alpha_{c}*\beta_{c} \\
    \eta_{c}:  &  \alpha_{c}*\frac{\sum_{s \in S} \eta_{s}*t_{s,c}} {|t_{c}|} \\
    \theta_{s,0}: & \theta_{s,0}*\frac {\sum_{c \in KC} \alpha_{c}*t_{s,c}} {|t_{s}|}
\end{aligned}
\end{equation}
\caption{Comparison of LFA parameters to seIRT parameters}
\end{mycapequ}
\end{comment}

\subsection{Combined Model}
The three models introduced above can all be encompassed by a more complex model.

\begin{equation}
P = \sigma(\sum_{c \in KC}\frac{\alpha_c \theta_{s,0}}{|KC|}+\eta_{s} \gamma_{c} g_{s,c} + \eta_{s}\rho_{c} f_{s,c} - \beta_{c})
\end{equation}

LFA can be obtained from this model by taking $\alpha=1$, $\eta=1$ and $\gamma=\rho$. PFA can be obtained from this model by taking $\alpha=1$, $\eta=1$ and $\theta_{0}=0$ (minus the last one for PFA+). The adapted eIRF can be obtained by taking $\gamma=\rho=\alpha$ and realizing that $\beta$ already incorporates $\alpha$.

\begin{comment}
\begin{mycapequ}
\begin{equation}
\begin{aligned}
    \gamma_{c}:  & \gamma_{c}*\frac{\sum_{s \in S} \eta_{s}*g_{s,c}} {|g_{c}|} \\
    \rho_{c}:  & \rho_{c}*\frac{\sum_{s \in S} \eta_{s}*f_{s,c}} {|f_{c}|} \\
    \theta_{s,0}: & \theta_{s,0}*\frac {\sum_{c \in KC} \alpha_{c}*t_{s,c}} {|t_{s}|}
\end{aligned}
\end{equation}
\caption{Comparison of PFA parameters to full model parameters}
\end{mycapequ}

\begin{mycapequ}
\begin{equation}
\begin{aligned}
    \eta_{s}: \eta_{s}* \frac{\sum_{c \in KC} \frac{\gamma_{c} g_{c} +  f_{c}}{\alpha_{c}}}{g_{c}+f_{c}} \\
    \beta_{c}:  & \rho_{c}*\frac{\sum_{s \in S} \eta_{s}*f_{s,c}} {|f_{c}|}
\end{aligned}
\end{equation}
\caption{Comparison of seIRT parameters to full model parameters}
\end{mycapequ}
\end{comment}



\section{Method}


\subsection{Model Performance}

\label{sec:perf}
The most common performance measure used in the context of ITS learning models is some measure of accuracy of model predictions on next-item student performance. Although this research looks into the values of model parameters and is less concerned with other measures of performance an accuracy measure is still used. The main rationale behind this relates to subquestion 4 as the accuracy of a model may hold some relationship to how well the parameters are matched. This would be especially useful as accuracy measures are relatively easy to obtain and are often already looked at in ITSs. 

The specific accuracy measure used for this research is A' (pronounced a-prime) \cite{modelreview}. For this measure two items are represented to the model, one which was answered correctly and one which was answered incorrectly. The model is used to determine which is which. The advantage of this method is that "values of A' are statistically comparable across models and data sets" \cite{modelreview}. \todo{there is a problem in using normal A' due to the dependencies of answers by the same student, there is a paper that deals with this, which should be incorporated}

Log likelihood of the data given a model is another measure that plays a large role in this specific context as this is what is maximized in fitting the model. Although the log likelihood value might not be very informative, it does give an indication of how well the model fits the data-set and in combination with the measured accuracy one can get an idea to what extent over-fitting occurs. To be able to compare this value over different runs it will be normalized by dividing by the number of data points.


\todo{currently this part is out of the question. I might bring up this discussion again:
Additionally a method that from IRT \cite{hambleton} will be used. After initially fitting the data will be split in two along skills, such that one set contains all the easiest skills and one will contain all the most difficult skills. Then the fitting will be done again on both new data-sets. If the assumptions of the model were correct the parameters for students found should be roughly the same from both sets. This procedure will be repeated, but then making a split on student initial skill and looking at the skill parameters.}

Although the primary way in which model parameters will be examined is described above, alternative methods will be used where applicable. Some data-sets contain additional information that can be used to more directly draw conclusions on whether found parameter values are meaningful. Two examples from the related research section are \cite{eirt} where expert opinions and an indication of what groups have higher initial knowledge were used and \cite{ktpfa} where a pre-test was used as an indicator of initial knowledge.

\subsection{Inherent Variance}
Inherent variance is the variance discussed in subquestion 1. In IRT the concept of a test information function is used for this. In short this function is defined for a set of items and (indirectly) gives the variance for any student knowledge parameter evaluated on these items. Baker (2001) describes that the found variance is the variance that would be observed on fitting a model where a student would answer this set of items over and over again, while every time forgetting he has seen these items. An equivalent can also be made to estimate the variance of item parameters or both parameter sets at the same time. The variance of these information functions is only dependent on the value of all the parameters involved and of the items asked to the student. The labels of the items are thus irrelevant for the information function.

Such a function has the drawback that this variance is only achieved in the limit. I.e. the actual variance approaches this variance closer and closer as the number of observations go up. Some of the datasets looked at here are rather small though. A simulated approach will thus be used to determine the inherent variance of the model. The information function will still be used though and compared to the empirically found inherent variance as the information function is relatively easy to obtain and could be accurate enough and play a role in answering subquestion 4. 

In the simulated approach, first the parameters are determined by fitting the model on the data (here the labels do have their influence on the result). The found parameters are then used to stochastically generate new labels for the data. This means that if the model predicts a .2 probability that the question is answered correctly, 20\% of the time the label will be 1 and 80\% of the time the label will be 0. In the PFA model this value will also influence further probabilities due to the different learning rates for correct and incorrect answers to questions. Multiple sets of labels are generated for the data in this fashion, after which the same model is fitted again on these sets. The inherent variance of every parameters is then estimated by calculating the variance of that parameter over the different trained models.

\subsection{Observed variance}

In order to get an idea of the variance of parameters in reality, data is split into multiple parts. The same model is fitted on each part of the data (from now on called a split) and the variance for each parameter can then be calculated over the different models. 

The focus here lies on the KC parameters. Also the splitting of the data should retain the way the data might be seen by an ITS in real life. Therefor the data is split on the basis of students: every split contains the data of some of the students and every student in a split does not occur in another split. The same split is done on the test set such that the model trained on the data of specific students is also tested on the test set of those particular students.

To investigate subquestion 2, different splits are made, with a decreasing number of students per split. It should be noted that this means that for the larger splits, a lower number of parameter values are present and that thus the second order variance will be higher.

\subsection{Representation of variance}
The variances discussed in the previous sections are not easily interpretable as it is not directly clear whether a variance is large or small for a particular parameter. To account for this problem the following reliability measure for every parameter value will be used:

\begin{equation}
\label{eq:reliability}
R(\beta_{n}) = 1-\frac{\sigma^{2}(\beta_n)}{\sigma^{2}(\beta_{n})+\sigma^{2}(\beta)}
\end{equation}

Where $\beta_{n}$ is the $\beta$ parameter of the n-th KC, $\sigma^{2}(\beta_{n})$ is its variance and $\sigma^{2}(\beta)$ is the variance over the $\beta$ parameter of all KCs in the model. The reliability can be calculated for both the inherent and observed variance and can be calculated for any parameter, although $\beta$ was taken as an example here.

\subsection{Domains}
\label{sec:domain}
In generating the data there are many other factors that might play a role in the performance of the models. Among those factors are the values that the parameters will be given, the distribution of knowledge components over the items, the ratio of students to items etc. As indicated in \ref{sec:RQ} these factors are not be explored methodically and extensively, but rather a few different datasets are used to represent some of the variation naturally found in this kind of data.

\subsection{Data cleaning}
\label{sec:cleaning}
Splitting the data may exacerbate some of the issues that can be encountered in fitting the model. One example is when all questions associated with a student or a KC are answered correctly or incorrectly. This makes the fitting algorithm want to assign infinite values to parameters. Another problem is when for a KC there is such a limited number of questions answered that learning rates cannot be estimated.

To prevent these issues the following steps are taken. On the whole dataset, students that answer all questions correctly or incorrectly are removed. In every split KC's for which every question is answered correctly or incorrectly is not taken into account for that split. Additionally if for a KC there is less than two questions answered correctly or incorrectly that KC is not taken into account for that split. This means that this KC is removed from the items and any item that no longer contains a KC is dropped from the data. It is assumed that this does not cause students to now have answered all questions correctly or incorrectly, but this assumption will be checked during the experiments to be certain.

\subsection{KC Experiment}
\subsubsection{The Experiment}
In detail the experiment for KC parameters consists of the following steps per model:
\begin{enumerate}
\item Clean the data by removing students according to section \ref{sec:cleaning}
\item Split the data in a increasing number of parts. Then for each part:
\begin{enumerate}
\item Clean the data by removing KCs according to section \ref{sec:cleaning}
\item Train the model and determine the variance for every parameter type
\item Determine performance on the test-set for that split
\item Determine the inherent variance through the Fisher matrix and through simulation
\item Normalize the found variances with the variances per parameter type found above
\end{enumerate}
\item Determine the variance per parameter over all the splits
\item Normalize the found variances with the average of variances per parameter type found above
\end{enumerate}

\subsubsection{Representing the Results}
Dit is een voorstel voor het representeren van de resultaten. Feedback, ideeen of suggesties worden erg gewaardeerd.
For every split into a number of parts and every model the following will be represented:
\begin{enumerate}
\item The average reliability for each parametertype (both inherent and observed \todo{ preferably the reliability of the difference between the two as well, even though that has not worked too well in the testrun I have shown, although I'm less than 90\% sure I did not make a mistake})
\item The average accuracy and normalized log likelihood
\item Histograms that display the reliabilities of the different parameters. Outliers can now be easily spotted.
\end{enumerate}
\todo{I'd also like to see to what extent inherent and total variance are related over the different splittings and models, as well as relations with accuracy and normalized log likelihood. correlation or spearmans R are two options I'm considering}

\todo{
\section{Outstanding issues}
\begin{itemize}
\item In evaluating will accuracy in general be looked at, or rather will a more ordinal scale approach be taken
\item Describe terms and use those consistently!
\item get a better idea how A' should be applied in this context
\item related work is a bit out of place as it is not very intelligible without information given later. Maybe move it to the end, or rather integrate it with other stuff?
\item 1-skill per item setting or multiple skill per item setting? -> depends on the next point, but it seems many are multi-dimensional
\item find good data-set(s)...
\end{itemize}
}
\bibliographystyle{alpha}   % this means that the order of references
			    % is determined by the order in which the
			    % \cite and \nocite commands appear
\bibliography{litlist}
\newpage
\appendix
\section{Glossary}
\todo{
Making a start with using terms consistently, plus I generally feel that a glossary would have helped me greatly in understanding papers etc.}
\begin{description}
  \item[Item] A problem (step) in the ITS to which a single answer can be given
  \item[Knowledge Component] A skill, piece of knowledge etc. that is associated with one or more items and in which students can have a level of competence
  \item[Question] An instance of an item
  \item[Skill] Level of an instance of a knowledge component for a particular student
\end{description}

\end{document}
